<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <title>Re: org.apache.hadoop.hbase.ZooKeeperConnectionException: java.io.IOException: Too many open files</title>
  <link rel="stylesheet" type="text/css" href="/archives/style.css" />
 </head>

 <body id="archives">
  <h1>hbase-user mailing list archives</h1>

  <h5>
<a href="http://mail-archives.apache.org/mod_mbox/" title="Back to the archives depot">Site index</a> &middot; <a href="/mod_mbox/hbase-user" title="Back to the list index">List index</a></h5>  <table class="static" id="msgview">
   <thead>
    <tr>
    <th class="title">Message view</th>
    <th class="nav"><a href="/mod_mbox/hbase-user/201108.mbox/%3cC9F02C88-A3F4-44AC-A207-4AA8330F1D2B@gmail.com%3e" title="Previous by date">&laquo;</a> <a href="/mod_mbox/hbase-user/201108.mbox/date" title="View messages sorted by date">Date</a> <a href="/mod_mbox/hbase-user/201108.mbox/%3cCADcMMgFAgYDvdvhVVcA65HRGPxr0KAPaw=N7NL0gJtsvbc71pQ@mail.gmail.com%3e" title="Next by date">&raquo;</a> &middot; <a href="/mod_mbox/hbase-user/201108.mbox/%3c3309B11B-180E-4D2D-9ACA-35FDEBB7CF54@salesforce.com%3e" title="Previous by thread">&laquo;</a> <a href="/mod_mbox/hbase-user/201108.mbox/thread" title="View messages sorted by thread">Thread</a> <a href="/mod_mbox/hbase-user/201108.mbox/%3cCADcMMgFAgYDvdvhVVcA65HRGPxr0KAPaw=N7NL0gJtsvbc71pQ@mail.gmail.com%3e" title="Next by thread">&raquo;</a></th>
   </tr>
   </thead>

   <tfoot>
    <tr>
    <th class="title"><a href="#archives">Top</a></th>
    <th class="nav"><a href="/mod_mbox/hbase-user/201108.mbox/%3cC9F02C88-A3F4-44AC-A207-4AA8330F1D2B@gmail.com%3e" title="Previous by date">&laquo;</a> <a href="/mod_mbox/hbase-user/201108.mbox/date" title="View messages sorted by date">Date</a> <a href="/mod_mbox/hbase-user/201108.mbox/%3cCADcMMgFAgYDvdvhVVcA65HRGPxr0KAPaw=N7NL0gJtsvbc71pQ@mail.gmail.com%3e" title="Next by date">&raquo;</a> &middot; <a href="/mod_mbox/hbase-user/201108.mbox/%3c3309B11B-180E-4D2D-9ACA-35FDEBB7CF54@salesforce.com%3e" title="Previous by thread">&laquo;</a> <a href="/mod_mbox/hbase-user/201108.mbox/thread" title="View messages sorted by thread">Thread</a> <a href="/mod_mbox/hbase-user/201108.mbox/%3cCADcMMgFAgYDvdvhVVcA65HRGPxr0KAPaw=N7NL0gJtsvbc71pQ@mail.gmail.com%3e" title="Next by thread">&raquo;</a></th>
   </tr>
   </tfoot>

   <tbody>
   <tr class="from">
    <td class="left">From</td>
    <td class="right">Shuja Rehman &lt;shujamug...@gmail.com&gt;</td>
   </tr>
   <tr class="subject">
    <td class="left">Subject</td>
    <td class="right">Re: org.apache.hadoop.hbase.ZooKeeperConnectionException: java.io.IOException: Too many open files</td>
   </tr>
   <tr class="date">
    <td class="left">Date</td>
    <td class="right">Wed, 10 Aug 2011 13:43:58 GMT</td>
   </tr>
   <tr class="contents"><td colspan="2"><pre>
Hi,
I have set the limits as explained in the following link.

https://ccp.cloudera.com/display/CDHDOC/HBase+Installation#HBaseInstallation-SettingUserLimitsforHBase

Now it works fine for some more jobs But after that, I am getting following
exception now.

11/08/10 06:33:07 INFO zookeeper.ZooKeeper: Initiating client connection,
connectString=10.0.3.85:2181 sessionTimeout=180000 watcher=hconnection
11/08/10 06:33:07 INFO zookeeper.ClientCnxn: Opening socket connection to
server /10.0.3.85:2181
11/08/10 06:33:07 INFO zookeeper.ClientCnxn: Socket connection established
to hadoop.zoniversal.com/10.0.3.85:2181, initiating session
11/08/10 06:33:07 INFO zookeeper.ClientCnxn: Session establishment complete
on server hadoop.zoniversal.com/10.0.3.85:2181, sessionid =
0x131628a4a072891, negotiated timeout = 40000
11/08/10 06:33:07 WARN mapred.JobClient: Use GenericOptionsParser for
parsing the arguments. Applications should implement Tool for the same.
11/08/10 06:33:07 WARN hdfs.DFSClient: DFSOutputStream ResponseProcessor
exception  for block blk_156159297298887768_25457java.io.IOException: Too
many open files
        at sun.nio.ch.IOUtil.initPipe(Native Method)
        at sun.nio.ch.EPollSelectorImpl.&lt;init&gt;(EPollSelectorImpl.java:49)
        at
sun.nio.ch.EPollSelectorProvider.openSelector(EPollSelectorProvider.java:18)
        at
org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.get(SocketIOWithTimeout.java:407)
        at
org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:322)
        at
org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:157)
        at
org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:155)
        at
org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:128)
        at java.io.DataInputStream.readFully(DataInputStream.java:178)
        at java.io.DataInputStream.readLong(DataInputStream.java:399)
        at
org.apache.hadoop.hdfs.protocol.DataTransferProtocol$PipelineAck.readFields(DataTransferProtocol.java:120)
        at
org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$ResponseProcessor.run(DFSClient.java:2638)

11/08/10 06:33:07 WARN hdfs.DFSClient: DataStreamer Exception:
java.io.InterruptedIOException: Interruped while waiting for IO on channel
java.nio.channels.SocketChannel[connected local=/10.0.3.85:57316 remote=/
10.0.3.85:50010]. 484999 millis timeout left.
        at
org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:349)
        at
org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:157)
        at
org.apache.hadoop.net.SocketOutputStream.write(SocketOutputStream.java:146)
        at
org.apache.hadoop.net.SocketOutputStream.write(SocketOutputStream.java:107)
        at java.io.BufferedOutputStream.write(BufferedOutputStream.java:105)
        at java.io.DataOutputStream.write(DataOutputStream.java:90)
        at
org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2526)

11/08/10 06:33:07 INFO hdfs.DFSClient: Error Recovery for block
blk_156159297298887768_25457 waiting for responder to exit.
11/08/10 06:33:07 WARN hdfs.DFSClient: Error Recovery for block
blk_156159297298887768_25457 bad datanode[0] 10.0.3.85:50010
11/08/10 06:33:07 INFO mapred.JobClient: Cleaning up the staging area
hdfs://
hadoop.zoniversal.com/var/lib/hadoop-0.20/cache/mapred/mapred/staging/root/.staging/job_201107251124_0619
Aug 10, 2011 6:33:08 AM hbaseaggregator.AggregatorDriver main
SEVERE: null
java.io.IOException: All datanodes 10.0.3.85:50010 are bad. Aborting...
        at
org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:2774)
        at
org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$1600(DFSClient.java:2305)
        at
org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2477)
11/08/10 06:33:08 ERROR hdfs.DFSClient: Exception closing file
/var/lib/hadoop-0.20/cache/mapred/mapred/staging/root/.staging/job_201107251124_0619/libjars/zookeeper-3.3.3-cdh3u0.jar
: java.io.IOException: All datanodes 10.0.3.85:50010 are bad. Aborting...
java.io.IOException: All datanodes 10.0.3.85:50010 are bad. Aborting...
        at
org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:2774)
        at
org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$1600(DFSClient.java:2305)
        at
org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2477)


kindly let me know the solution.
thnx

On Tue, Aug 9, 2011 at 8:38 PM, Ian Varley &lt;ivarley@salesforce.com&gt; wrote:

&gt; Hi Shuja,
&gt;
&gt; This question is mentioned in the HBase FAQ, here:
&gt;
&gt; http://wiki.apache.org/hadoop/Hbase/FAQ_Operations#A3
&gt;
&gt; which points to the HBase book:
&gt;
&gt; http://hbase.apache.org/book.html#ulimit
&gt;
&gt; "HBase is a database. It uses a lot of files all at the same time. The
&gt; default ulimit -n -- i.e. user file limit -- of 1024 on most *nix systems is
&gt; insufficient (On mac os x its 256). Any significant amount of loading will
&gt; lead you to "FAQ: Why do I see "java.io.IOException...(Too many open files)"
&gt; in my logs?" ... Do yourself a favor and change the upper bound on the
&gt; number of file descriptors. Set it to north of 10k. See the above referenced
&gt; FAQ for how. You should also up the hbase users' nproc setting; under load,
&gt; a low-nproc setting could manifest as OutOfMemoryError. To be clear, upping
&gt; the file descriptors and nproc for the user who is running the HBase process
&gt; is an operating system configuration, not an HBase configuration. Also, a
&gt; common mistake is that administrators will up the file descriptors for a
&gt; particular user but for whatever reason, HBase will be running as some one
&gt; else. HBase prints in its logs as the first line the ulimit its seeing.
&gt; Ensure its correct."
&gt;
&gt; - Ian
&gt;
&gt; On Aug 9, 2011, at 10:32 AM, Shuja Rehman wrote:
&gt;
&gt; Hi All,
&gt;
&gt; I am running map reduce jobs in queue and after some jobs, the following
&gt; exception starts to come. Let me know if I need to change any settings or
&gt; anything else
&gt;
&gt; Thanks in advance.
&gt;
&gt;
&gt; 11/08/09 06:19:56 INFO mapreduce.TableOutputFormat: Created table instance
&gt; for mytable
&gt; 11/08/09 06:19:56 INFO zookeeper.ZooKeeper: Initiating client connection,
&gt; connectString=10.0.3.85:2181 sessionTimeout=180000 watcher=hconnection
&gt; 11/08/09 06:19:56 ERROR mapreduce.TableInputFormat:
&gt; org.apache.hadoop.hbase.ZooKeeperConnectionException: java.io.IOException:
&gt; Too many open files
&gt;       at
&gt;
&gt; org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.getZooKeeperWatcher(HConnectionManager.java:991)
&gt;       at
&gt;
&gt; org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.setupZookeeperTrackers(HConnectionManager.java:302)
&gt;       at
&gt;
&gt; org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.&lt;init&gt;(HConnectionManager.java:293)
&gt;       at
&gt;
&gt; org.apache.hadoop.hbase.client.HConnectionManager.getConnection(HConnectionManager.java:156)
&gt;       at org.apache.hadoop.hbase.client.HTable.&lt;init&gt;(HTable.java:167)
&gt;       at org.apache.hadoop.hbase.client.HTable.&lt;init&gt;(HTable.java:145)
&gt;       at
&gt;
&gt; org.apache.hadoop.hbase.mapreduce.TableInputFormat.setConf(TableInputFormat.java:91)
&gt;       at
&gt; org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:62)
&gt;       at
&gt;
&gt; org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:117)
&gt;       at
&gt; org.apache.hadoop.mapred.JobClient.writeNewSplits(JobClient.java:941)
&gt;       at
&gt; org.apache.hadoop.mapred.JobClient.writeSplits(JobClient.java:961)
&gt;       at org.apache.hadoop.mapred.JobClient.access$500(JobClient.java:170)
&gt;       at org.apache.hadoop.mapred.JobClient$2.run(JobClient.java:880)
&gt;       at org.apache.hadoop.mapred.JobClient$2.run(JobClient.java:833)
&gt;       at java.security.AccessController.doPrivileged(Native Method)
&gt;       at javax.security.auth.Subject.doAs(Subject.java:396)
&gt;       at
&gt;
&gt; org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1115)
&gt;       at
&gt; org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:833)
&gt;       at org.apache.hadoop.mapreduce.Job.submit(Job.java:476)
&gt;       at org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:506)
&gt;       at
&gt; hbaseaggregator.AggregatorRunner.runJob(AggregatorRunner.java:242)
&gt;       at
&gt; hbaseaggregator.AggregatorRunner.Aggregator(AggregatorRunner.java:180)
&gt;       at hbaseaggregator.AggregatorDriver.main(AggregatorDriver.java:60)
&gt;       at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
&gt;       at
&gt;
&gt; sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
&gt;       at
&gt;
&gt; sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
&gt;       at java.lang.reflect.Method.invoke(Method.java:597)
&gt;       at org.apache.hadoop.util.RunJar.main(RunJar.java:186)
&gt; Caused by: java.io.IOException: Too many open files
&gt;       at sun.nio.ch.IOUtil.initPipe(Native Method)
&gt;       at sun.nio.ch.EPollSelectorImpl.&lt;init&gt;(EPollSelectorImpl.java:49)
&gt;       at
&gt;
&gt; sun.nio.ch.EPollSelectorProvider.openSelector(EPollSelectorProvider.java:18)
&gt;       at java.nio.channels.Selector.open(Selector.java:209)
&gt;       at org.apache.zookeeper.ClientCnxn.&lt;init&gt;(ClientCnxn.java:160)
&gt;       at org.apache.zookeeper.ClientCnxn.&lt;init&gt;(ClientCnxn.java:331)
&gt;       at org.apache.zookeeper.ZooKeeper.&lt;init&gt;(ZooKeeper.java:377)
&gt;       at org.apache.hadoop.hbase.zookeeper.ZKUtil.connect(ZKUtil.java:97)
&gt;       at
&gt;
&gt; org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.&lt;init&gt;(ZooKeeperWatcher.java:119)
&gt;       at
&gt;
&gt; org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.getZooKeeperWatcher(HConnectionManager.java:989)
&gt;       ... 27 more
&gt;
&gt; 11/08/09 06:19:56 INFO mapred.JobClient: Cleaning up the staging area
&gt; hdfs://
&gt;
&gt; hadoop.zoniversal.com/var/lib/hadoop-0.20/cache/mapred/mapred/staging/root/.staging/job_201107251124_0383
&gt; &lt;
&gt; http://hadoop.zoniversal.com/var/lib/hadoop-0.20/cache/mapred/mapred/staging/root/.staging/job_201107251124_0383
&gt; &gt;
&gt; Exception in thread "main" java.lang.InternalError
&gt;       at
&gt; sun.misc.URLClassPath$JarLoader.getResource(URLClassPath.java:755)
&gt;       at sun.misc.URLClassPath.getResource(URLClassPath.java:169)
&gt;       at java.net.URLClassLoader$1.run(URLClassLoader.java:194)
&gt;       at java.security.AccessController.doPrivileged(Native Method)
&gt;       at java.net.URLClassLoader.findClass(URLClassLoader.java:190)
&gt;       at sun.misc.Launcher$ExtClassLoader.findClass(Launcher.java:229)
&gt;       at java.lang.ClassLoader.loadClass(ClassLoader.java:307)
&gt;       at java.lang.ClassLoader.loadClass(ClassLoader.java:296)
&gt;       at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)
&gt;       at java.lang.ClassLoader.loadClass(ClassLoader.java:248)
&gt;       at
&gt; java.util.ResourceBundle$RBClassLoader.loadClass(ResourceBundle.java:435)
&gt;       at
&gt; java.util.ResourceBundle$Control.newBundle(ResourceBundle.java:2289)
&gt;       at java.util.ResourceBundle.loadBundle(ResourceBundle.java:1364)
&gt;       at java.util.ResourceBundle.findBundle(ResourceBundle.java:1328)
&gt;       at java.util.ResourceBundle.findBundle(ResourceBundle.java:1282)
&gt;       at java.util.ResourceBundle.getBundleImpl(ResourceBundle.java:1224)
&gt;       at java.util.ResourceBundle.getBundle(ResourceBundle.java:705)
&gt;       at java.util.logging.Level.getLocalizedName(Level.java:223)
&gt;       at java.util.logging.SimpleFormatter.format(SimpleFormatter.java:64)
&gt;       at java.util.logging.StreamHandler.publish(StreamHandler.java:179)
&gt;       at java.util.logging.ConsoleHandler.publish(ConsoleHandler.java:88)
&gt;       at java.util.logging.Logger.log(Logger.java:458)
&gt;       at java.util.logging.Logger.doLog(Logger.java:480)
&gt;       at java.util.logging.Logger.log(Logger.java:569)
&gt;       at hbaseaggregator.AggregatorDriver.main(AggregatorDriver.java:72)
&gt;       at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
&gt;       at
&gt;
&gt; sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
&gt;       at
&gt;
&gt; sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
&gt;       at java.lang.reflect.Method.invoke(Method.java:597)
&gt;       at org.apache.hadoop.util.RunJar.main(RunJar.java:186)
&gt; Caused by: java.util.zip.ZipException: error in opening zip file
&gt;       at java.util.zip.ZipFile.open(Native Method)
&gt;       at java.util.zip.ZipFile.&lt;init&gt;(ZipFile.java:114)
&gt;       at java.util.jar.JarFile.&lt;init&gt;(JarFile.java:135)
&gt;       at java.util.jar.JarFile.&lt;init&gt;(JarFile.java:72)
&gt;       at sun.misc.URLClassPath$JarLoader.getJarFile(URLClassPath.java:646)
&gt;       at sun.misc.URLClassPath$JarLoader.access$600(URLClassPath.java:540)
&gt;       at sun.misc.URLClassPath$JarLoader$1.run(URLClassPath.java:607)
&gt;       at java.security.AccessController.doPrivileged(Native Method)
&gt;       at sun.misc.URLClassPath$JarLoader.ensureOpen(URLClassPath.java:599)
&gt;       at
&gt; sun.misc.URLClassPath$JarLoader.getResource(URLClassPath.java:753)
&gt;       ... 29 more
&gt;
&gt; --
&gt; Regards
&gt; Shuja-ur-Rehman Baig
&gt; &lt;http://pk.linkedin.com/in/shujamughal&gt;
&gt;
&gt;


-- 
Regards
Shuja-ur-Rehman Baig
&lt;http://pk.linkedin.com/in/shujamughal&gt;

</pre></td></tr>
   <tr class="mime">
    <td class="left">Mime</td>
    <td class="right">
<ul>
<li>Unnamed multipart/alternative (inline, None, 0 bytes)</li>
<ul>
<li><a rel="nofollow" href="/mod_mbox/hbase-user/201108.mbox/raw/%3cCALMfpjq4iXpGk_70ehz9YuOVP7BYme1b=-KyW-dh6tTiRZBH6w@mail.gmail.com%3e/1">Unnamed text/plain</a> (inline, None, 13282 bytes)</li>
</ul>
</ul>
</td>
</tr>
   <tr class="raw">
    <td class="left"></td>
    <td class="right"><a href="/mod_mbox/hbase-user/201108.mbox/raw/%3cCALMfpjq4iXpGk_70ehz9YuOVP7BYme1b=-KyW-dh6tTiRZBH6w@mail.gmail.com%3e" rel="nofollow">View raw message</a></td>
   </tr>
   </tbody>
  </table>
 </body>
</html>
