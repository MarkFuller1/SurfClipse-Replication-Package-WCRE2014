<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8" /><title>[HBase-user]  org.apache.hadoop.hbase.ZooKeeperConnectionException: java.io.IOException: Too many open files - Grokbase</title>
<script type="text/javascript" src="http://ajax.googleapis.com/ajax/libs/jquery/1.6.3/jquery.min.js"></script><script type="text/javascript" src="/cc/gb2.js"></script>
<link rel="stylesheet" type="text/css" href="/cc/gb2.css" />
<link rel="canonical" href="/t/hbase/user/118902mpra/org-apache-hadoop-hbase-zookeeperconnectionexception-java-io-ioexception-too-many-open-files" /><meta name="description" content="(3 replies) Hi All, I am running map reduce jobs in queue and after some jobs, the following exception starts to come. Let me know if I need to change any settings or anything else Thanks in advance. 11/08/09 06:19:56 INFO mapreduce.TableOutputFormat: Created table instance for mytable 11/08/09 06:19:56 INFO zookeeper.ZooKeeper: Initiating client connection, connectString=10.0.3.85:2181 sessionTimeout=180000 watcher=hconnection 11/08/09 06:19:56 ERROR mapreduce.TableInputFormat:" /><link rel="alternate" type="application/rss+xml" title="RSS feed for user @ hbase.apache.org" href="http://grokbase.com/g/hbase/user/feed" /><link rel="alternate" type="application/rss+xml" title="RSS feed for this discussion" href="http://grokbase.com/t/hbase/user/118902mpra/org-apache-hadoop-hbase-zookeeperconnectionexception-java-io-ioexception-too-many-open-files/feed" /><link rel="stylesheet" type="text/css" href="/cc/badges20.css" /><link rel="stylesheet" type="text/css" href="/cc/teambadges20.css" /><script type="text/javascript" src="/cc/jq/p/jquery.hotkeys.js"></script><link rel="stylesheet" type="text/css" href="/cc/jq/qtip2/jquery.qtip2.master.css" />
<link rel="stylesheet" type="text/css" href="/cc/jq/qtip2/jquery.qtip2.min.css" />
<script type="text/javascript" src="/cc/jq/qtip2/jquery.qtip2.min.js"></script>
<script type="text/javascript">
$(document).ready(function() {
	$('a[href^="/user/"]').each(function() {
		var url='/ucard/' + $(this).attr('href').split('/')[3] + '/user@hbase.apache.org';
		$(this).qtip({content:{text:'<img class="throbber" src="/cc/img/anim.throbber.gif" alt="Loading..." />',ajax:{url:url}},position:{at:'bottom center',my:'top center',viewport:$(window),effect:false},show:{solo:true},style:{classes:'ui-tooltip-wiki ui-tooltip-light ui-tooltip-shadow'}})
	})
});
</script>
	</head>
<body itemscope itemtype="http://schema.org/WebPage"><a name="top"></a>
<div id="msgbar0" class="msgbar0" style="display:none"><div id="msgbar1" class="msgbar1"></div></div><div style="clear:both;"></div>
<div id="topbar">
	<div id="topbar_content">
		<div id="topbar_message">
							<div itemprop="breadcrumb"><a href="/">Grokbase</a> <span class="inactive">&rsaquo;</span> <a href="/groups">Groups</a> <span class="inactive">&rsaquo;</span> <a href="/s/hbase">HBase</a> <span class="inactive">&rsaquo;</span> <a href="/g/hbase/user">user</a> <span class="inactive">&rsaquo;</span> <a href="/g/hbase/user/2011/08">August 2011</a></div>			</div>
		<div id="topbar_search"><form class="topbar_search" action="/search" method="get"><input onclick="if (this.value=='search'){this.value='';return false;}" onblur="if (this.value==''){this.value='search';return false}" name="q" id="q" class="topbar_search" type="text" value="search" /></form></div>
				<div id="topbar_navbar"><a class="topbar_nav" href="/faq">FAQ</a> <script type="text/javascript">
				gbOSch();
				</script></div>
	</div>
</div>
<div class="main100">

	<div class="sobar">
		<div class="fr">
<div class="fb-like" data-href="http://grokbase.com" data-send="false" data-layout="button_count" data-show-faces="true" data-action="recommend"></div>
<div><g:plusone size="small" href="http://grokbase.com"></g:plusone></div>
		</div>
	</div>
	<div style="clear:both;"></div>

	<div class="mainbar_so">
		<div id="logo"><a href="/"><img src="/cc/img/logo_grokbase_ff.png" /></a></div>
		<div id="nav0">
			<div class="nav0">
				<ul>
					<li class="youarehere"><a id="nav0_groups" href="/groups">Groups</a></li>
					<li><a id="nav0_users" href="/users">Users</a></li>
					<li><a id="nav0_badges" href="/badges">Badges</a></li>
					<!-- <li><a id="nav0_badges" href="/news">News</a></li> -->
				</ul>
			</div>
		</div>
	</div>

	<div style="clear:both;"></div>
<div id="fb-root"></div>
	<div class="main">
	<div class="sup2header"><h1 class="sup2header_text hl_y">[HBase-user]  org.apache.hadoop.hbase.ZooKeeperConnectionException: java.io.IOException: Too many open files</h1>

	<div class="sup2buttons"><a href="http://grokbase.com/g/hbase/user/feed" title="RSS feed for user @ hbase.apache.org"><img src="/ico/sbj/rss/24" /></a> <a href="/t/hbase/user/118902mpra/org-apache-hadoop-hbase-zookeeperconnectionexception-java-io-ioexception-too-many-open-files/feed" title="RSS feed for this dicussion"><img src="/ico/sbj/rss/24" /></a></div>
	</div>
	<div class="main_1">

						<div itemscope itemtype="http://schema.org/Article" class="highlightable" rel="20110809acuwyk3tipeksengahwzthalai">
			<div style="padding:0 0 0.5em 0">
<table style="font-size:100%" border="0"><tr><td style="vertical-align:top" rowspan="2"><a href="/user/Shuja-Rehman/Yf1fAmN5vPScoUqpokfi3Y" title="Shuja Rehman"><img class="av_md" src="/avatar/Yf1fAmN5vPScoUqpokfi3Y/40" alt="Shuja Rehman" width="40" height="40" /></a></td><td style="padding-left:0.5em;vertical-align:top;white-space:nowrap"><span style="margin-top:-5px"><a href="/user/Shuja-Rehman/Yf1fAmN5vPScoUqpokfi3Y"><span itemprop="author" itemscope itemtype="http://schema.org/Person"><span itemprop="name" class="hl_y">Shuja Rehman</span><meta itemprop="url" content="/user/Shuja-Rehman/Yf1fAmN5vPScoUqpokfi3Y"/></span></a></span></td><td style="width:100%"><div style="width:40px"><a href="/badges/hbase"><div title="Bronze HBase badge (99 posts)" class="u_badge badge_tn badge_tn_3_hbase"></div></a><a href="/badges/hadoop"><div title="Bronze Hadoop badge (188 posts)" class="u_badge badge_tn badge_tn_3_hadoop"></div></a></div></td></tr><tr><td style="padding-left:0.5em" colspan="2"><a class="dt_y4" href="/g/hbase/user/2011/08">Aug 9, 2011 at 3:32 pm</a></td>
</tr></table>
			</div>
			<div style="clear:both"></div>
			<meta itemprop="name" content=""/>
			<meta itemprop="interactionCount" content="UserComments:3"/>
			<div class="m_body m_body_one hl_y" itemprop="articleBody">Hi All,<br /><br />I am running map reduce jobs in queue and after some jobs, the following<br />exception starts to come. Let me know if I need to change any settings or<br />anything else<br /><br />Thanks in advance.<br /><br /><br />11/08/09 06:19:56 INFO mapreduce.TableOutputFormat: Created table instance<br />for mytable<br />11/08/09 06:19:56 INFO zookeeper.ZooKeeper: Initiating client connection,<br />connectString=10.0.3.85:2181 sessionTimeout=180000 watcher=hconnection<br />11/08/09 06:19:56 ERROR mapreduce.TableInputFormat:<br />org.apache.hadoop.hbase.ZooKeeperConnectionException: java.io.IOException:<br />Too many open files<br />at<br />org.apache.hadoop.hbase.client.HConnectionManager&#36;HConnectionImplementation.getZooKeeperWatcher(HConnectionManager.java:991)<br />at<br />org.apache.hadoop.hbase.client.HConnectionManager&#36;HConnectionImplementation.setupZookeeperTrackers(HConnectionManager.java:302)<br />at<br />org.apache.hadoop.hbase.client.HConnectionManager&#36;HConnectionImplementation.(HConnectionManager.java:156)<br />at org.apache.hadoop.hbase.client.HTable.(HTable.java:145)<br />at<br />org.apache.hadoop.hbase.mapreduce.TableInputFormat.setConf(TableInputFormat.java:91)<br />at<br />org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:62)<br />at<br />org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:117)<br />at<br />org.apache.hadoop.mapred.JobClient.writeNewSplits(JobClient.java:941)<br />at<br />org.apache.hadoop.mapred.JobClient.writeSplits(JobClient.java:961)<br />at org.apache.hadoop.mapred.JobClient.access&#36;500(JobClient.java:170)<br />at org.apache.hadoop.mapred.JobClient&#36;2.run(JobClient.java:880)<br />at org.apache.hadoop.mapred.JobClient&#36;2.run(JobClient.java:833)<br />at java.security.AccessController.doPrivileged(Native Method)<br />at javax.security.auth.Subject.doAs(Subject.java:396)<br />at<br />org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1115)<br />at<br />org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:833)<br />at org.apache.hadoop.mapreduce.Job.submit(Job.java:476)<br />at org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:506)<br />at<br />hbaseaggregator.AggregatorRunner.runJob(AggregatorRunner.java:242)<br />at<br />hbaseaggregator.AggregatorRunner.Aggregator(AggregatorRunner.java:180)<br />at hbaseaggregator.AggregatorDriver.main(AggregatorDriver.java:60)<br />at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)<br />at<br />sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)<br />at<br />sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)<br />at java.lang.reflect.Method.invoke(Method.java:597)<br />at org.apache.hadoop.util.RunJar.main(RunJar.java:186)<br />Caused by: java.io.IOException: Too many open files<br />at sun.nio.ch.IOUtil.initPipe(Native Method)<br />at sun.nio.ch.EPollSelectorImpl.(EPollSelectorProvider.java:18)<br />at java.nio.channels.Selector.open(Selector.java:209)<br />at org.apache.zookeeper.ClientCnxn.(ClientCnxn.java:331)<br />at org.apache.zookeeper.ZooKeeper.(ZKUtil.java:97)<br />at<br />org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.(HConnectionManager.java:989)<br />... 27 more<br /><br />11/08/09 06:19:56 INFO mapred.JobClient: Cleaning up the staging area<br />hdfs://<br />hadoop.zoniversal.com/var/lib/hadoop-0.20/cache/mapred/mapred/staging/root/.staging/job_201107251124_0383<br />Exception in thread &quot;main&quot; java.lang.InternalError<br />at<br />sun.misc.URLClassPath&#36;JarLoader.getResource(URLClassPath.java:755)<br />at sun.misc.URLClassPath.getResource(URLClassPath.java:169)<br />at java.net.URLClassLoader&#36;1.run(URLClassLoader.java:194)<br />at java.security.AccessController.doPrivileged(Native Method)<br />at java.net.URLClassLoader.findClass(URLClassLoader.java:190)<br />at sun.misc.Launcher&#36;ExtClassLoader.findClass(Launcher.java:229)<br />at java.lang.ClassLoader.loadClass(ClassLoader.java:307)<br />at java.lang.ClassLoader.loadClass(ClassLoader.java:296)<br />at sun.misc.Launcher&#36;AppClassLoader.loadClass(Launcher.java:301)<br />at java.lang.ClassLoader.loadClass(ClassLoader.java:248)<br />at<br />java.util.ResourceBundle&#36;RBClassLoader.loadClass(ResourceBundle.java:435)<br />at<br />java.util.ResourceBundle&#36;Control.newBundle(ResourceBundle.java:2289)<br />at java.util.ResourceBundle.loadBundle(ResourceBundle.java:1364)<br />at java.util.ResourceBundle.findBundle(ResourceBundle.java:1328)<br />at java.util.ResourceBundle.findBundle(ResourceBundle.java:1282)<br />at java.util.ResourceBundle.getBundleImpl(ResourceBundle.java:1224)<br />at java.util.ResourceBundle.getBundle(ResourceBundle.java:705)<br />at java.util.logging.Level.getLocalizedName(Level.java:223)<br />at java.util.logging.SimpleFormatter.format(SimpleFormatter.java:64)<br />at java.util.logging.StreamHandler.publish(StreamHandler.java:179)<br />at java.util.logging.ConsoleHandler.publish(ConsoleHandler.java:88)<br />at java.util.logging.Logger.log(Logger.java:458)<br />at java.util.logging.Logger.doLog(Logger.java:480)<br />at java.util.logging.Logger.log(Logger.java:569)<br />at hbaseaggregator.AggregatorDriver.main(AggregatorDriver.java:72)<br />at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)<br />at<br />sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)<br />at<br />sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)<br />at java.lang.reflect.Method.invoke(Method.java:597)<br />at org.apache.hadoop.util.RunJar.main(RunJar.java:186)<br />Caused by: java.util.zip.ZipException: error in opening zip file<br />at java.util.zip.ZipFile.open(Native Method)<br />at java.util.zip.ZipFile.(JarFile.java:135)<br />at java.util.jar.JarFile.(URLClassPath.java:646)<br />at sun.misc.URLClassPath&#36;JarLoader.access&#36;600(URLClassPath.java:540)<br />at sun.misc.URLClassPath&#36;JarLoader&#36;1.run(URLClassPath.java:607)<br />at java.security.AccessController.doPrivileged(Native Method)<br />at sun.misc.URLClassPath&#36;JarLoader.ensureOpen(URLClassPath.java:599)<br />at<br />sun.misc.URLClassPath&#36;JarLoader.getResource(URLClassPath.java:753)<br />... 29 more<br /><a id="qc_20110809acuwyk3tipeksengahwzthalai_z__ctl" class="qc_ctl" href="#"></a><div id="qc_20110809acuwyk3tipeksengahwzthalai_z" class="qc_cnt"><br />--<br />Regards<br />Shuja-ur-Rehman Baig<br /><<a href="http://pk.linkedin.com/in/shujamughal" rel="nofollow">http://pk.linkedin.com/in/shujamughal</a>></div></div>
			<div style="margin-top:1em"><a href="mailto:user%40hbase.apache.org?In-Reply-To=%3CCALMfpjoxpa7K6OkzMdnAGHdFjfJqXi_J4a%2Bdu_RdRzNQZ%2B2hXg%40mail.gmail.com%3E&Subject=org.apache.hadoop.hbase.ZooKeeperConnectionException%3A%20java.io.IOException%3A%20Too%20many%20open%20files">reply</a></div>
			<div style="clear:both"></div>
		</div>

<script type="text/javascript">
reddit_target='hadoop';
</script><table border="0" style="margin:0;padding:4px 0 0 0;">
<tr><td style="text-align:top">
<div class="fb-like" data-href="http://grokbase.com/p/hbase/user/118902mpra" data-send="false" data-layout="box_count" data-show-faces="false"></div>
</td><td style="text-align:top">
<a href="https://twitter.com/share" class="twitter-share-button" data-count="vertical">Tweet</a>
</td><td style="padding-left:10px;text-align:top">
<g:plusone size="tall"></g:plusone>
</td><td style="padding-left:10px;text-align:top">
<script type="text/javascript" src="http://www.reddit.com/static/button/button2.js"></script>
</td></tr>
</table>
<script type="text/javascript" src="//platform.twitter.com/widgets.js"></script>

		
		<div id="search_discussions" style="margin:0"><h2 class="section">Search Discussions</h2>			<form action="/search" id="gcse_g_search">
			<div>
				<input type="text" name="q" id="txtSearch" style="width: 400px;" maxlength="255" value="" />
				<input type="submit" id="btnSearch" value="Search" />
			</div>
			<input id="sch1_gfa" type="radio" name="f" value="aa/site" /> <label for="sch1_gfa">All Groups</label> <input id ="sch1_gf" type="radio" name="f" value="gt/hbase/user" checked="checked" /> <label for="sch1_gf">user</label>						</form>
		</div>
		<div style="clear:both;height:1em"></div>
		
		<a name="responses_tab_top"></a>
		<div class="subheader">
			<h2 class="subheader">3 responses</h2>
			<div id="tabs">
				<a id="sort_nested" href="/t/hbase/user/118902mpra/org-apache-hadoop-hbase-zookeeperconnectionexception-java-io-ioexception-too-many-open-files#responses_tab_top" class="sortTab youarehere">Nested</a>
				<a id="sort_nested" href="/t/hbase/user/118902mpra/org-apache-hadoop-hbase-zookeeperconnectionexception-java-io-ioexception-too-many-open-files/oldest#responses_tab_top" class="sortTab">Oldest</a>
			</div>
		</div><div style="clear:both"></div>
		<ul id="responses" class="nobullet">
		
		<li id="20110809gekxbcnneysxen3vzrjq2imbi4_li" class="responses">
		<div itemscope itemtype="http://schema.org/Article">
			<a id="20110809gekxbcnneysxen3vzrjq2imbi4" name="118964aq12" title="20110809gekxbcnneysxen3vzrjq2imbi4,,"></a>
			<meta itemprop="name" content=""/>

			<table border="0" id="20110809gekxbcnneysxen3vzrjq2imbi4_thinhead" class="tr_head_thin rc_ctl_thin hl_y" style="display:none"><tr><td id="20110809gekxbcnneysxen3vzrjq2imbi4_th_indent" class="indent1 indentd0" style="width:20px;margin:0;padding:0 0 0 0px"><img class="av_sm" src="/avatar/3BtKyz56CyauKGRrdHYYm3/20" class="avatar tr_avatar" alt="Ian Varley" />
				</td><td class="tr_head_content tr_head_thin_content tr_head_content_right" style="width:627px;overflow:hidden"><div class="tr_head_thin_content">
				<strong>Ian Varley</strong> <span itemprop="articleBody">Hi Shuja, This question is mentioned in the HBase FAQ, here: http://wiki.apache.org/hadoop/Hbase/FAQ_Operations#A3 which points to the HBase book: http://hbase.apache.org/book.html#ulimit &quot;HBase is a database. It uses a lot of files all at the same time. The default ulimit -n -- i.e. user file limit -- of 1024 on most *nix systems is insufficient (On mac os x its 256). Any significant amount of loading will lead you to &quot;FAQ: Why do I see &quot;java.io.IOException...(Too many open files)&quot; in my</span></div>
			</td><td><div style="width:40px"><a href="/badges/hbase"><div title="Bronze HBase badge (92 posts)" class="u_badge badge_tn badge_tn_3_hbase"></div></a><a href="/badges/hadoop"><div title="Bronze Hadoop badge (92 posts)" class="u_badge badge_tn badge_tn_3_hadoop"></div></a></div></td></tr></table>

			<table id="20110809gekxbcnneysxen3vzrjq2imbi4_fullhead" class="rbody tr_head_full"><tr><td id="20110809gekxbcnneysxen3vzrjq2imbi4_fh_indent" class="indent1 indentd0" style="width:20px;margin:0;padding:0 0 0 0px"><a href="/user/Ian-Varley/3BtKyz56CyauKGRrdHYYm3"><img class="av_sm" src="/avatar/3BtKyz56CyauKGRrdHYYm3/20" class="avatar tr_avatar" alt="Ian Varley" /></a>				</td><td class="tr_head_content tr_head_full_content_user">
				<strong><span itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="url" content="/user/Ian-Varley/3BtKyz56CyauKGRrdHYYm3"/><a class="tr_head_user" href="/user/Ian-Varley/3BtKyz56CyauKGRrdHYYm3"><span class="hl_y" itemprop="name">Ian Varley</span></a></span></strong> </td>
				<td id="20110809gekxbcnneysxen3vzrjq2imbi4_fullhead_ctl" class="tr_head_content tr_head_content_right rc_ctl_full" style='padding-top:2px'> at <span class="dt_y4 relativetime">Aug 9, 2011 at 3:39 pm</span>
			</td><td>
			<a href="#top">&#8679;</a></td><td style="margin-top:0px"><a href="#20110809gekxbcnneysxen3vzrjq2imbi4"><img src="/cc/img/nav/icon_anchor_16x16.png" /></a>
			</td><td><div style="width:40px"><a href="/badges/hbase"><div title="Bronze HBase badge (92 posts)" class="u_badge badge_tn badge_tn_3_hbase"></div></a><a href="/badges/hadoop"><div title="Bronze Hadoop badge (92 posts)" class="u_badge badge_tn badge_tn_3_hadoop"></div></a></div></td></tr></table>

			<div id="20110809gekxbcnneysxen3vzrjq2imbi4_content" class="rbody m_body indent2 indentd0 tr_content hl_y" style="padding-left:30px;">
				Hi Shuja,<br /><br />This question is mentioned in the HBase FAQ, here:<br /><br /><a href="http://wiki.apache.org/hadoop/Hbase/FAQ_Operations#A3" rel="nofollow">http://wiki.apache.org/hadoop/Hbase/FAQ_Operations#A3</a><br /><br />which points to the HBase book:<br /><br /><a href="http://hbase.apache.org/book.html#ulimit" rel="nofollow">http://hbase.apache.org/book.html#ulimit</a><br /><br />&quot;HBase is a database. It uses a lot of files all at the same time. The default ulimit -n -- i.e. user file limit -- of 1024 on most *nix systems is insufficient (On mac os x its 256). Any significant amount of loading will lead you to &quot;FAQ: Why do I see &quot;java.io.IOException...(Too many open files)&quot; in my logs?&quot; ... Do yourself a favor and change the upper bound on the number of file descriptors. Set it to north of 10k. See the above referenced FAQ for how. You should also up the hbase users&#39; nproc setting; under load, a low-nproc setting could manifest as OutOfMemoryError. To be clear, upping the file descriptors and nproc for the user who is running the HBase process is an operating system configuration, not an HBase configuration. Also, a common mistake is that administrators will up the file descriptors for a particular user but for whatever reason, HBase will be running as some one else. HBase prints in its logs as the first line the ulimit its seeing. Ensure its correct.&quot;<br /><br />- Ian<br /><br />On Aug 9, 2011, at 10:32 AM, Shuja Rehman wrote:<br /><br />Hi All,<br /><br />I am running map reduce jobs in queue and after some jobs, the following<br />exception starts to come. Let me know if I need to change any settings or<br />anything else<br /><br />Thanks in advance.<br /><br /><br />11/08/09 06:19:56 INFO mapreduce.TableOutputFormat: Created table instance<br />for mytable<br />11/08/09 06:19:56 INFO zookeeper.ZooKeeper: Initiating client connection,<br />connectString=10.0.3.85:2181 sessionTimeout=180000 watcher=hconnection<br />11/08/09 06:19:56 ERROR mapreduce.TableInputFormat:<br />org.apache.hadoop.hbase.ZooKeeperConnectionException: java.io.IOException:<br />Too many open files<br />at<br />org.apache.hadoop.hbase.client.HConnectionManager&#36;HConnectionImplementation.getZooKeeperWatcher(HConnectionManager.java:991)<br />at<br />org.apache.hadoop.hbase.client.HConnectionManager&#36;HConnectionImplementation.setupZookeeperTrackers(HConnectionManager.java:302)<br />at<br />org.apache.hadoop.hbase.client.HConnectionManager&#36;HConnectionImplementation.(HConnectionManager.java:156)<br />at org.apache.hadoop.hbase.client.HTable.(HTable.java:145)<br />at<br />org.apache.hadoop.hbase.mapreduce.TableInputFormat.setConf(TableInputFormat.java:91)<br />at<br />org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:62)<br />at<br />org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:117)<br />at<br />org.apache.hadoop.mapred.JobClient.writeNewSplits(JobClient.java:941)<br />at<br />org.apache.hadoop.mapred.JobClient.writeSplits(JobClient.java:961)<br />at org.apache.hadoop.mapred.JobClient.access&#36;500(JobClient.java:170)<br />at org.apache.hadoop.mapred.JobClient&#36;2.run(JobClient.java:880)<br />at org.apache.hadoop.mapred.JobClient&#36;2.run(JobClient.java:833)<br />at java.security.AccessController.doPrivileged(Native Method)<br />at javax.security.auth.Subject.doAs(Subject.java:396)<br />at<br />org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1115)<br />at<br />org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:833)<br />at org.apache.hadoop.mapreduce.Job.submit(Job.java:476)<br />at org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:506)<br />at<br />hbaseaggregator.AggregatorRunner.runJob(AggregatorRunner.java:242)<br />at<br />hbaseaggregator.AggregatorRunner.Aggregator(AggregatorRunner.java:180)<br />at hbaseaggregator.AggregatorDriver.main(AggregatorDriver.java:60)<br />at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)<br />at<br />sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)<br />at<br />sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)<br />at java.lang.reflect.Method.invoke(Method.java:597)<br />at org.apache.hadoop.util.RunJar.main(RunJar.java:186)<br />Caused by: java.io.IOException: Too many open files<br />at sun.nio.ch.IOUtil.initPipe(Native Method)<br />at sun.nio.ch.EPollSelectorImpl.(EPollSelectorProvider.java:18)<br />at java.nio.channels.Selector.open(Selector.java:209)<br />at org.apache.zookeeper.ClientCnxn.(ClientCnxn.java:331)<br />at org.apache.zookeeper.ZooKeeper.(ZKUtil.java:97)<br />at<br />org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.(HConnectionManager.java:989)<br />... 27 more<br /><br />11/08/09 06:19:56 INFO mapred.JobClient: Cleaning up the staging area<br />hdfs://<br />hadoop.zoniversal.com/var/lib/hadoop-0.20/cache/mapred/mapred/staging/root/.staging/job_201107251124_0383<<a href="http://hadoop.zoniversal.com/var/lib/hadoop-0.20/cache/mapred/mapred/staging/root/.staging/job_201107251124_0383" rel="nofollow">http://hadoop.zoniversal.com/var/lib/hadoop-0.20/cache/mapred/mapred/staging/root/.staging/job_201107251124_0383</a>><br />Exception in thread &quot;main&quot; java.lang.InternalError<br />at<br />sun.misc.URLClassPath&#36;JarLoader.getResource(URLClassPath.java:755)<br />at sun.misc.URLClassPath.getResource(URLClassPath.java:169)<br />at java.net.URLClassLoader&#36;1.run(URLClassLoader.java:194)<br />at java.security.AccessController.doPrivileged(Native Method)<br />at java.net.URLClassLoader.findClass(URLClassLoader.java:190)<br />at sun.misc.Launcher&#36;ExtClassLoader.findClass(Launcher.java:229)<br />at java.lang.ClassLoader.loadClass(ClassLoader.java:307)<br />at java.lang.ClassLoader.loadClass(ClassLoader.java:296)<br />at sun.misc.Launcher&#36;AppClassLoader.loadClass(Launcher.java:301)<br />at java.lang.ClassLoader.loadClass(ClassLoader.java:248)<br />at<br />java.util.ResourceBundle&#36;RBClassLoader.loadClass(ResourceBundle.java:435)<br />at<br />java.util.ResourceBundle&#36;Control.newBundle(ResourceBundle.java:2289)<br />at java.util.ResourceBundle.loadBundle(ResourceBundle.java:1364)<br />at java.util.ResourceBundle.findBundle(ResourceBundle.java:1328)<br />at java.util.ResourceBundle.findBundle(ResourceBundle.java:1282)<br />at java.util.ResourceBundle.getBundleImpl(ResourceBundle.java:1224)<br />at java.util.ResourceBundle.getBundle(ResourceBundle.java:705)<br />at java.util.logging.Level.getLocalizedName(Level.java:223)<br />at java.util.logging.SimpleFormatter.format(SimpleFormatter.java:64)<br />at java.util.logging.StreamHandler.publish(StreamHandler.java:179)<br />at java.util.logging.ConsoleHandler.publish(ConsoleHandler.java:88)<br />at java.util.logging.Logger.log(Logger.java:458)<br />at java.util.logging.Logger.doLog(Logger.java:480)<br />at java.util.logging.Logger.log(Logger.java:569)<br />at hbaseaggregator.AggregatorDriver.main(AggregatorDriver.java:72)<br />at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)<br />at<br />sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)<br />at<br />sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)<br />at java.lang.reflect.Method.invoke(Method.java:597)<br />at org.apache.hadoop.util.RunJar.main(RunJar.java:186)<br />Caused by: java.util.zip.ZipException: error in opening zip file<br />at java.util.zip.ZipFile.open(Native Method)<br />at java.util.zip.ZipFile.(JarFile.java:135)<br />at java.util.jar.JarFile.(URLClassPath.java:646)<br />at sun.misc.URLClassPath&#36;JarLoader.access&#36;600(URLClassPath.java:540)<br />at sun.misc.URLClassPath&#36;JarLoader&#36;1.run(URLClassPath.java:607)<br />at java.security.AccessController.doPrivileged(Native Method)<br />at sun.misc.URLClassPath&#36;JarLoader.ensureOpen(URLClassPath.java:599)<br />at<br />sun.misc.URLClassPath&#36;JarLoader.getResource(URLClassPath.java:753)<br />... 29 more<br /><a id="qc_20110809gekxbcnneysxen3vzrjq2imbi4_z__ctl" class="qc_ctl" href="#"></a><div id="qc_20110809gekxbcnneysxen3vzrjq2imbi4_z" class="qc_cnt"><br />--<br />Regards<br />Shuja-ur-Rehman Baig<br /><<a href="http://pk.linkedin.com/in/shujamughal" rel="nofollow">http://pk.linkedin.com/in/shujamughal</a>></div>				<div style="margin-top:1em">
					<a href="mailto:user%40hbase.apache.org?In-Reply-To=%3C3309B11B-180E-4D2D-9ACA-35FDEBB7CF54%40salesforce.com%3E&Subject=Re%3A%20org.apache.hadoop.hbase.ZooKeeperConnectionException%3A%20java.io.IOException%3A%20Too%20many%20open%20files">reply</a> <span class="inactive">|</span> <a href="/p/hbase/user/118964aq12/org-apache-hadoop-hbase-zookeeperconnectionexception-java-io-ioexception-too-many-open-files">permalink</a>
				</div>
			</div>

		</div><!-- so.article -->
		<div style="clear:both"></div>
		</li>

		
		<li id="2011081032g5adwrm3oi7dc5ucnyrfmjbm_li" class="responses">
		<div itemscope itemtype="http://schema.org/Article">
			<a id="2011081032g5adwrm3oi7dc5ucnyrfmjbm" name="118avt6x03" title="2011081032g5adwrm3oi7dc5ucnyrfmjbm,,"></a>
			<meta itemprop="name" content=""/>

			<table border="0" id="2011081032g5adwrm3oi7dc5ucnyrfmjbm_thinhead" class="tr_head_thin rc_ctl_thin hl_y" style="display:none"><tr><td id="2011081032g5adwrm3oi7dc5ucnyrfmjbm_th_indent" class="indent1 indentd1" style="width:20px;margin:0;padding:0 0 0 16px"><img class="av_sm" src="/avatar/Yf1fAmN5vPScoUqpokfi3Y/20" class="avatar tr_avatar" alt="Shuja Rehman" />
				</td><td class="tr_head_content tr_head_thin_content tr_head_content_right" style="width:591px;overflow:hidden"><div class="tr_head_thin_content">
				<strong>Shuja Rehman</strong> <span itemprop="articleBody">Hi, I have set the limits as explained in the following link. https://ccp.cloudera.com/display/CDHDOC/HBase+Installation#HBaseInstallation-SettingUserLimitsforHBase Now it works fine for some more jobs But after that, I am getting following exception now. 11/08/10 06:33:07 INFO zookeeper.ZooKeeper: Initiating client connection, connectString=10.0.3.85:2181 sessionTimeout=180000 watcher=hconnection 11/08/10 06:33:07 INFO zookeeper.ClientCnxn: Opening socket connection to server /10.0.3.85:2181</span></div>
			</td><td><div style="width:60px"><a href="/badges/hbase"><div title="Bronze HBase badge (99 posts)" class="u_badge badge_tn badge_tn_3_hbase"></div></a><a href="/badges/hadoop"><div title="Bronze Hadoop badge (188 posts)" class="u_badge badge_tn badge_tn_3_hadoop"></div></a><div title="Original poster / discussion starter" class="u_badge badge_tn badge_tn_1_infoop"></div></div></td></tr></table>

			<table id="2011081032g5adwrm3oi7dc5ucnyrfmjbm_fullhead" class="rbody tr_head_full"><tr><td id="2011081032g5adwrm3oi7dc5ucnyrfmjbm_fh_indent" class="indent1 indentd1" style="width:20px;margin:0;padding:0 0 0 16px"><a href="/user/Shuja-Rehman/Yf1fAmN5vPScoUqpokfi3Y"><img class="av_sm" src="/avatar/Yf1fAmN5vPScoUqpokfi3Y/20" class="avatar tr_avatar" alt="Shuja Rehman" /></a>				</td><td class="tr_head_content tr_head_full_content_user">
				<strong><span itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="url" content="/user/Shuja-Rehman/Yf1fAmN5vPScoUqpokfi3Y"/><a class="tr_head_user" href="/user/Shuja-Rehman/Yf1fAmN5vPScoUqpokfi3Y"><span class="hl_y" itemprop="name">Shuja Rehman</span></a></span></strong> </td>
				<td id="2011081032g5adwrm3oi7dc5ucnyrfmjbm_fullhead_ctl" class="tr_head_content tr_head_content_right rc_ctl_full" style='padding-top:2px'> at <span class="dt_y4 relativetime">Aug 10, 2011 at 1:44 pm</span>
			</td><td>
			<a href="#top">&#8679;</a></td><td style="margin-top:0px"><a href="#2011081032g5adwrm3oi7dc5ucnyrfmjbm"><img src="/cc/img/nav/icon_anchor_16x16.png" /></a>
			</td><td><div style="width:60px"><a href="/badges/hbase"><div title="Bronze HBase badge (99 posts)" class="u_badge badge_tn badge_tn_3_hbase"></div></a><a href="/badges/hadoop"><div title="Bronze Hadoop badge (188 posts)" class="u_badge badge_tn badge_tn_3_hadoop"></div></a><div title="Original poster / discussion starter" class="u_badge badge_tn badge_tn_1_infoop"></div></div></td></tr></table>

			<div id="2011081032g5adwrm3oi7dc5ucnyrfmjbm_content" class="rbody m_body indent2 indentd1 tr_content hl_y" style="padding-left:46px;">
				Hi,<br />I have set the limits as explained in the following link.<br /><br /><a href="https://ccp.cloudera.com/display/CDHDOC/HBase+Installation#HBaseInstallation-SettingUserLimitsforHBase" rel="nofollow">https://ccp.cloudera.com/display/CDHDOC/HBase+Installation#HBaseInstallation-SettingUserLimitsforHBase</a><br /><br />Now it works fine for some more jobs But after that, I am getting following<br />exception now.<br /><br />11/08/10 06:33:07 INFO zookeeper.ZooKeeper: Initiating client connection,<br />connectString=10.0.3.85:2181 sessionTimeout=180000 watcher=hconnection<br />11/08/10 06:33:07 INFO zookeeper.ClientCnxn: Opening socket connection to<br />server /10.0.3.85:2181<br />11/08/10 06:33:07 INFO zookeeper.ClientCnxn: Socket connection established<br />to hadoop.zoniversal.com/10.0.3.85:2181, initiating session<br />11/08/10 06:33:07 INFO zookeeper.ClientCnxn: Session establishment complete<br />on server hadoop.zoniversal.com/10.0.3.85:2181, sessionid =<br />0x131628a4a072891, negotiated timeout = 40000<br />11/08/10 06:33:07 WARN mapred.JobClient: Use GenericOptionsParser for<br />parsing the arguments. Applications should implement Tool for the same.<br />11/08/10 06:33:07 WARN hdfs.DFSClient: DFSOutputStream ResponseProcessor<br />exception  for block blk_156159297298887768_25457java.io.IOException: Too<br />many open files<br />at sun.nio.ch.IOUtil.initPipe(Native Method)<br />at sun.nio.ch.EPollSelectorImpl.(EPollSelectorProvider.java:18)<br />at<br />org.apache.hadoop.net.SocketIOWithTimeout&#36;SelectorPool.get(SocketIOWithTimeout.java:407)<br />at<br />org.apache.hadoop.net.SocketIOWithTimeout&#36;SelectorPool.select(SocketIOWithTimeout.java:322)<br />at<br />org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:157)<br />at<br />org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:155)<br />at<br />org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:128)<br />at java.io.DataInputStream.readFully(DataInputStream.java:178)<br />at java.io.DataInputStream.readLong(DataInputStream.java:399)<br />at<br />org.apache.hadoop.hdfs.protocol.DataTransferProtocol&#36;PipelineAck.readFields(DataTransferProtocol.java:120)<br />at<br />org.apache.hadoop.hdfs.DFSClient&#36;DFSOutputStream&#36;ResponseProcessor.run(DFSClient.java:2638)<br /><br />11/08/10 06:33:07 WARN hdfs.DFSClient: DataStreamer Exception:<br />java.io.InterruptedIOException: Interruped while waiting for IO on channel<br />java.nio.channels.SocketChannel[connected local=/10.0.3.85:57316 remote=/<br />10.0.3.85:50010]. 484999 millis timeout left.<br />at<br />org.apache.hadoop.net.SocketIOWithTimeout&#36;SelectorPool.select(SocketIOWithTimeout.java:349)<br />at<br />org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:157)<br />at<br />org.apache.hadoop.net.SocketOutputStream.write(SocketOutputStream.java:146)<br />at<br />org.apache.hadoop.net.SocketOutputStream.write(SocketOutputStream.java:107)<br />at java.io.BufferedOutputStream.write(BufferedOutputStream.java:105)<br />at java.io.DataOutputStream.write(DataOutputStream.java:90)<br />at<br />org.apache.hadoop.hdfs.DFSClient&#36;DFSOutputStream&#36;DataStreamer.run(DFSClient.java:2526)<br /><br />11/08/10 06:33:07 INFO hdfs.DFSClient: Error Recovery for block<br />blk_156159297298887768_25457 waiting for responder to exit.<br />11/08/10 06:33:07 WARN hdfs.DFSClient: Error Recovery for block<br />blk_156159297298887768_25457 bad datanode[0] 10.0.3.85:50010<br />11/08/10 06:33:07 INFO mapred.JobClient: Cleaning up the staging area<br />hdfs://<br />hadoop.zoniversal.com/var/lib/hadoop-0.20/cache/mapred/mapred/staging/root/.staging/job_201107251124_0619<br />Aug 10, 2011 6:33:08 AM hbaseaggregator.AggregatorDriver main<br />SEVERE: null<br />java.io.IOException: All datanodes 10.0.3.85:50010 are bad. Aborting...<br />at<br />org.apache.hadoop.hdfs.DFSClient&#36;DFSOutputStream.processDatanodeError(DFSClient.java:2774)<br />at<br />org.apache.hadoop.hdfs.DFSClient&#36;DFSOutputStream.access&#36;1600(DFSClient.java:2305)<br />at<br />org.apache.hadoop.hdfs.DFSClient&#36;DFSOutputStream&#36;DataStreamer.run(DFSClient.java:2477)<br />11/08/10 06:33:08 ERROR hdfs.DFSClient: Exception closing file<br />/var/lib/hadoop-0.20/cache/mapred/mapred/staging/root/.staging/job_201107251124_0619/libjars/zookeeper-3.3.3-cdh3u0.jar<br />: java.io.IOException: All datanodes 10.0.3.85:50010 are bad. Aborting...<br />java.io.IOException: All datanodes 10.0.3.85:50010 are bad. Aborting...<br />at<br />org.apache.hadoop.hdfs.DFSClient&#36;DFSOutputStream.processDatanodeError(DFSClient.java:2774)<br />at<br />org.apache.hadoop.hdfs.DFSClient&#36;DFSOutputStream.access&#36;1600(DFSClient.java:2305)<br />at<br />org.apache.hadoop.hdfs.DFSClient&#36;DFSOutputStream&#36;DataStreamer.run(DFSClient.java:2477)<br /><br /><br />kindly let me know the solution.<br />thnx<br /><a id="qc_2011081032g5adwrm3oi7dc5ucnyrfmjbm_z__ctl" class="qc_ctl" href="#"></a><div id="qc_2011081032g5adwrm3oi7dc5ucnyrfmjbm_z" class="qc_cnt"><div class="ql ql0"><span class="qc_wrote">On Tue, Aug 9, 2011 at 8:38 PM, Ian Varley wrote:</span><br /><br />Hi Shuja,<br /><br />This question is mentioned in the HBase FAQ, here:<br /><br /><a href="http://wiki.apache.org/hadoop/Hbase/FAQ_Operations#A3" rel="nofollow">http://wiki.apache.org/hadoop/Hbase/FAQ_Operations#A3</a><br /><br />which points to the HBase book:<br /><br /><a href="http://hbase.apache.org/book.html#ulimit" rel="nofollow">http://hbase.apache.org/book.html#ulimit</a><br /><br />&quot;HBase is a database. It uses a lot of files all at the same time. The<br />default ulimit -n -- i.e. user file limit -- of 1024 on most *nix systems is<br />insufficient (On mac os x its 256). Any significant amount of loading will<br />lead you to &quot;FAQ: Why do I see &quot;java.io.IOException...(Too many open files)&quot;<br />in my logs?&quot; ... Do yourself a favor and change the upper bound on the<br />number of file descriptors. Set it to north of 10k. See the above referenced<br />FAQ for how. You should also up the hbase users&#39; nproc setting; under load,<br />a low-nproc setting could manifest as OutOfMemoryError. To be clear, upping<br />the file descriptors and nproc for the user who is running the HBase process<br />is an operating system configuration, not an HBase configuration. Also, a<br />common mistake is that administrators will up the file descriptors for a<br />particular user but for whatever reason, HBase will be running as some one<br />else. HBase prints in its logs as the first line the ulimit its seeing.<br />Ensure its correct.&quot;<br /><br />- Ian<br /><br />On Aug 9, 2011, at 10:32 AM, Shuja Rehman wrote:<br /><br />Hi All,<br /><br />I am running map reduce jobs in queue and after some jobs, the following<br />exception starts to come. Let me know if I need to change any settings or<br />anything else<br /><br />Thanks in advance.<br /><br /><br />11/08/09 06:19:56 INFO mapreduce.TableOutputFormat: Created table instance<br />for mytable<br />11/08/09 06:19:56 INFO zookeeper.ZooKeeper: Initiating client connection,<br />connectString=10.0.3.85:2181 sessionTimeout=180000 watcher=hconnection<br />11/08/09 06:19:56 ERROR mapreduce.TableInputFormat:<br />org.apache.hadoop.hbase.ZooKeeperConnectionException: java.io.IOException:<br />Too many open files<br />at<br /><br />org.apache.hadoop.hbase.client.HConnectionManager&#36;HConnectionImplementation.getZooKeeperWatcher(HConnectionManager.java:991)<br />at<br /><br />org.apache.hadoop.hbase.client.HConnectionManager&#36;HConnectionImplementation.setupZookeeperTrackers(HConnectionManager.java:302)<br />at<br /><br />org.apache.hadoop.hbase.client.HConnectionManager&#36;HConnectionImplementation.&lt;init&gt;(HConnectionManager.java:293)<br />at<br /><br />org.apache.hadoop.hbase.client.HConnectionManager.getConnection(HConnectionManager.java:156)<br />at org.apache.hadoop.hbase.client.HTable.&lt;init&gt;(HTable.java:167)<br />at org.apache.hadoop.hbase.client.HTable.&lt;init&gt;(HTable.java:145)<br />at<br /><br />org.apache.hadoop.hbase.mapreduce.TableInputFormat.setConf(TableInputFormat.java:91)<br />at<br />org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:62)<br />at<br /><br />org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:117)<br />at<br />org.apache.hadoop.mapred.JobClient.writeNewSplits(JobClient.java:941)<br />at<br />org.apache.hadoop.mapred.JobClient.writeSplits(JobClient.java:961)<br />at org.apache.hadoop.mapred.JobClient.access&#36;500(JobClient.java:170)<br />at org.apache.hadoop.mapred.JobClient&#36;2.run(JobClient.java:880)<br />at org.apache.hadoop.mapred.JobClient&#36;2.run(JobClient.java:833)<br />at java.security.AccessController.doPrivileged(Native Method)<br />at javax.security.auth.Subject.doAs(Subject.java:396)<br />at<br /><br />org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1115)<br />at<br />org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:833)<br />at org.apache.hadoop.mapreduce.Job.submit(Job.java:476)<br />at org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:506)<br />at<br />hbaseaggregator.AggregatorRunner.runJob(AggregatorRunner.java:242)<br />at<br />hbaseaggregator.AggregatorRunner.Aggregator(AggregatorRunner.java:180)<br />at hbaseaggregator.AggregatorDriver.main(AggregatorDriver.java:60)<br />at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)<br />at<br /><br />sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)<br />at<br /><br />sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)<br />at java.lang.reflect.Method.invoke(Method.java:597)<br />at org.apache.hadoop.util.RunJar.main(RunJar.java:186)<br />Caused by: java.io.IOException: Too many open files<br />at sun.nio.ch.IOUtil.initPipe(Native Method)<br />at sun.nio.ch.EPollSelectorImpl.&lt;init&gt;(EPollSelectorImpl.java:49)<br />at<br /><br />sun.nio.ch.EPollSelectorProvider.openSelector(EPollSelectorProvider.java:18)<br />at java.nio.channels.Selector.open(Selector.java:209)<br />at org.apache.zookeeper.ClientCnxn.&lt;init&gt;(ClientCnxn.java:160)<br />at org.apache.zookeeper.ClientCnxn.&lt;init&gt;(ClientCnxn.java:331)<br />at org.apache.zookeeper.ZooKeeper.&lt;init&gt;(ZooKeeper.java:377)<br />at org.apache.hadoop.hbase.zookeeper.ZKUtil.connect(ZKUtil.java:97)<br />at<br /><br />org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.&lt;init&gt;(ZooKeeperWatcher.java:119)<br />at<br /><br />org.apache.hadoop.hbase.client.HConnectionManager&#36;HConnectionImplementation.getZooKeeperWatcher(HConnectionManager.java:989)<br />... 27 more<br /><br />11/08/09 06:19:56 INFO mapred.JobClient: Cleaning up the staging area<br />hdfs://<br /><br />hadoop.zoniversal.com/var/lib/hadoop-0.20/cache/mapred/mapred/staging/root/.staging/job_201107251124_0383<br />&lt;<br /><a href="http://hadoop.zoniversal.com/var/lib/hadoop-0.20/cache/mapred/mapred/staging/root/.staging/job_201107251124_0383" rel="nofollow">http://hadoop.zoniversal.com/var/lib/hadoop-0.20/cache/mapred/mapred/staging/root/.staging/job_201107251124_0383</a><br /><div class="ql ql1"></div>Exception in thread &quot;main&quot; java.lang.InternalError<br />at<br />sun.misc.URLClassPath&#36;JarLoader.getResource(URLClassPath.java:755)<br />at sun.misc.URLClassPath.getResource(URLClassPath.java:169)<br />at java.net.URLClassLoader&#36;1.run(URLClassLoader.java:194)<br />at java.security.AccessController.doPrivileged(Native Method)<br />at java.net.URLClassLoader.findClass(URLClassLoader.java:190)<br />at sun.misc.Launcher&#36;ExtClassLoader.findClass(Launcher.java:229)<br />at java.lang.ClassLoader.loadClass(ClassLoader.java:307)<br />at java.lang.ClassLoader.loadClass(ClassLoader.java:296)<br />at sun.misc.Launcher&#36;AppClassLoader.loadClass(Launcher.java:301)<br />at java.lang.ClassLoader.loadClass(ClassLoader.java:248)<br />at<br />java.util.ResourceBundle&#36;RBClassLoader.loadClass(ResourceBundle.java:435)<br />at<br />java.util.ResourceBundle&#36;Control.newBundle(ResourceBundle.java:2289)<br />at java.util.ResourceBundle.loadBundle(ResourceBundle.java:1364)<br />at java.util.ResourceBundle.findBundle(ResourceBundle.java:1328)<br />at java.util.ResourceBundle.findBundle(ResourceBundle.java:1282)<br />at java.util.ResourceBundle.getBundleImpl(ResourceBundle.java:1224)<br />at java.util.ResourceBundle.getBundle(ResourceBundle.java:705)<br />at java.util.logging.Level.getLocalizedName(Level.java:223)<br />at java.util.logging.SimpleFormatter.format(SimpleFormatter.java:64)<br />at java.util.logging.StreamHandler.publish(StreamHandler.java:179)<br />at java.util.logging.ConsoleHandler.publish(ConsoleHandler.java:88)<br />at java.util.logging.Logger.log(Logger.java:458)<br />at java.util.logging.Logger.doLog(Logger.java:480)<br />at java.util.logging.Logger.log(Logger.java:569)<br />at hbaseaggregator.AggregatorDriver.main(AggregatorDriver.java:72)<br />at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)<br />at<br /><br />sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)<br />at<br /><br />sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)<br />at java.lang.reflect.Method.invoke(Method.java:597)<br />at org.apache.hadoop.util.RunJar.main(RunJar.java:186)<br />Caused by: java.util.zip.ZipException: error in opening zip file<br />at java.util.zip.ZipFile.open(Native Method)<br />at java.util.zip.ZipFile.&lt;init&gt;(ZipFile.java:114)<br />at java.util.jar.JarFile.&lt;init&gt;(JarFile.java:135)<br />at java.util.jar.JarFile.&lt;init&gt;(JarFile.java:72)<br />at sun.misc.URLClassPath&#36;JarLoader.getJarFile(URLClassPath.java:646)<br />at sun.misc.URLClassPath&#36;JarLoader.access&#36;600(URLClassPath.java:540)<br />at sun.misc.URLClassPath&#36;JarLoader&#36;1.run(URLClassPath.java:607)<br />at java.security.AccessController.doPrivileged(Native Method)<br />at sun.misc.URLClassPath&#36;JarLoader.ensureOpen(URLClassPath.java:599)<br />at<br />sun.misc.URLClassPath&#36;JarLoader.getResource(URLClassPath.java:753)<br />... 29 more<br /><br />--<br />Regards<br />Shuja-ur-Rehman Baig<br /><<a href="http://pk.linkedin.com/in/shujamughal" rel="nofollow">http://pk.linkedin.com/in/shujamughal</a>><br /></div><br />--<br />Regards<br />Shuja-ur-Rehman Baig<br /><<a href="http://pk.linkedin.com/in/shujamughal" rel="nofollow">http://pk.linkedin.com/in/shujamughal</a>></div>				<div style="margin-top:1em">
					<a href="mailto:user%40hbase.apache.org?In-Reply-To=%3CCALMfpjq4iXpGk_70ehz9YuOVP7BYme1b%3D-KyW-dh6tTiRZBH6w%40mail.gmail.com%3E&Subject=Re%3A%20org.apache.hadoop.hbase.ZooKeeperConnectionException%3A%20java.io.IOException%3A%20Too%20many%20open%20files">reply</a> <span class="inactive">|</span> <a href="/p/hbase/user/118avt6x03/org-apache-hadoop-hbase-zookeeperconnectionexception-java-io-ioexception-too-many-open-files">permalink</a>
				</div>
			</div>

		</div><!-- so.article -->
		<div style="clear:both"></div>
		</li>

		
		<li id="20110810slwj3zq2slqkgy252sfeumhule_li" class="responses">
		<div itemscope itemtype="http://schema.org/Article">
			<a id="20110810slwj3zq2slqkgy252sfeumhule" name="118ajbp9vs" title="20110810slwj3zq2slqkgy252sfeumhule,,"></a>
			<meta itemprop="name" content=""/>

			<table border="0" id="20110810slwj3zq2slqkgy252sfeumhule_thinhead" class="tr_head_thin rc_ctl_thin hl_y" style="display:none"><tr><td id="20110810slwj3zq2slqkgy252sfeumhule_th_indent" class="indent1 indentd2" style="width:20px;margin:0;padding:0 0 0 32px"><img class="av_sm" src="/avatar/jzPsrKPBmnffhPqbsNH3ST/20" class="avatar tr_avatar" alt="Stack" />
				</td><td class="tr_head_content tr_head_thin_content tr_head_content_right" style="width:555px;overflow:hidden"><div class="tr_head_thin_content">
				<strong>Stack</strong> <span itemprop="articleBody">You must not have set it everywhere. Its same issue as yesterday. St.Ack</span></div>
			</td><td><div style="width:80px"><a href="/badges/hbase"><div title="Gold HBase badge (6,142 posts)" class="u_badge badge_tn badge_tn_1_hbase"></div></a><a href="/badges/hadoop"><div title="Gold Hadoop badge (6,397 posts)" class="u_badge badge_tn badge_tn_1_hadoop"></div></a><a href="/team/apache-hbase"><div title="Apache HBase - Members" class="u_badge teambadge_tn teambadge_tn_1_apache-hbase"></div></a><a href="/team/apache-hadoop"><div title="Apache Hadoop - PMC" class="u_badge teambadge_tn teambadge_tn_1_apache-hadoop"></div></a></div></td></tr></table>

			<table id="20110810slwj3zq2slqkgy252sfeumhule_fullhead" class="rbody tr_head_full"><tr><td id="20110810slwj3zq2slqkgy252sfeumhule_fh_indent" class="indent1 indentd2" style="width:20px;margin:0;padding:0 0 0 32px"><a href="/user/Stack/jzPsrKPBmnffhPqbsNH3ST"><img class="av_sm" src="/avatar/jzPsrKPBmnffhPqbsNH3ST/20" class="avatar tr_avatar" alt="Stack" /></a>				</td><td class="tr_head_content tr_head_full_content_user">
				<strong><span itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="url" content="/user/Stack/jzPsrKPBmnffhPqbsNH3ST"/><a class="tr_head_user" href="/user/Stack/jzPsrKPBmnffhPqbsNH3ST"><span class="hl_y" itemprop="name">Stack</span></a></span></strong> </td>
				<td id="20110810slwj3zq2slqkgy252sfeumhule_fullhead_ctl" class="tr_head_content tr_head_content_right rc_ctl_full" style='padding-top:2px'> at <span class="dt_y4 relativetime">Aug 10, 2011 at 3:56 pm</span>
			</td><td>
			<a href="#top">&#8679;</a></td><td style="margin-top:0px"><a href="#20110810slwj3zq2slqkgy252sfeumhule"><img src="/cc/img/nav/icon_anchor_16x16.png" /></a>
			</td><td><div style="width:80px"><a href="/badges/hbase"><div title="Gold HBase badge (6,142 posts)" class="u_badge badge_tn badge_tn_1_hbase"></div></a><a href="/badges/hadoop"><div title="Gold Hadoop badge (6,397 posts)" class="u_badge badge_tn badge_tn_1_hadoop"></div></a><a href="/team/apache-hbase"><div title="Apache HBase - Members" class="u_badge teambadge_tn teambadge_tn_1_apache-hbase"></div></a><a href="/team/apache-hadoop"><div title="Apache Hadoop - PMC" class="u_badge teambadge_tn teambadge_tn_1_apache-hadoop"></div></a></div></td></tr></table>

			<div id="20110810slwj3zq2slqkgy252sfeumhule_content" class="rbody m_body indent2 indentd2 tr_content hl_y" style="padding-left:62px;">
				You must not have set it everywhere.  Its same issue as yesterday.<br />St.Ack<br /><a id="qc_20110810slwj3zq2slqkgy252sfeumhule_z__ctl" class="qc_ctl" href="#"></a><div id="qc_20110810slwj3zq2slqkgy252sfeumhule_z" class="qc_cnt"><div class="ql ql0"><span class="qc_wrote">On Wed, Aug 10, 2011 at 6:43 AM, Shuja Rehman wrote:</span><br />Hi,<br />I have set the limits as explained in the following link.<br /><br /><a href="https://ccp.cloudera.com/display/CDHDOC/HBase+Installation#HBaseInstallation-SettingUserLimitsforHBase" rel="nofollow">https://ccp.cloudera.com/display/CDHDOC/HBase+Installation#HBaseInstallation-SettingUserLimitsforHBase</a><br /><br />Now it works fine for some more jobs But after that, I am getting following<br />exception now.<br /><br />11/08/10 06:33:07 INFO zookeeper.ZooKeeper: Initiating client connection,<br />connectString=10.0.3.85:2181 sessionTimeout=180000 watcher=hconnection<br />11/08/10 06:33:07 INFO zookeeper.ClientCnxn: Opening socket connection to<br />server /10.0.3.85:2181<br />11/08/10 06:33:07 INFO zookeeper.ClientCnxn: Socket connection established<br />to hadoop.zoniversal.com/10.0.3.85:2181, initiating session<br />11/08/10 06:33:07 INFO zookeeper.ClientCnxn: Session establishment complete<br />on server hadoop.zoniversal.com/10.0.3.85:2181, sessionid =<br />0x131628a4a072891, negotiated timeout = 40000<br />11/08/10 06:33:07 WARN mapred.JobClient: Use GenericOptionsParser for<br />parsing the arguments. Applications should implement Tool for the same.<br />11/08/10 06:33:07 WARN hdfs.DFSClient: DFSOutputStream ResponseProcessor<br />exception &nbsp;for block blk_156159297298887768_25457java.io.IOException: Too<br />many open files<br />at sun.nio.ch.IOUtil.initPipe(Native Method)<br />at sun.nio.ch.EPollSelectorImpl.&lt;init&gt;(EPollSelectorImpl.java:49)<br />at<br />sun.nio.ch.EPollSelectorProvider.openSelector(EPollSelectorProvider.java:18)<br />at<br />org.apache.hadoop.net.SocketIOWithTimeout&#36;SelectorPool.get(SocketIOWithTimeout.java:407)<br />at<br />org.apache.hadoop.net.SocketIOWithTimeout&#36;SelectorPool.select(SocketIOWithTimeout.java:322)<br />at<br />org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:157)<br />at<br />org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:155)<br />at<br />org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:128)<br />at java.io.DataInputStream.readFully(DataInputStream.java:178)<br />at java.io.DataInputStream.readLong(DataInputStream.java:399)<br />at<br />org.apache.hadoop.hdfs.protocol.DataTransferProtocol&#36;PipelineAck.readFields(DataTransferProtocol.java:120)<br />at<br />org.apache.hadoop.hdfs.DFSClient&#36;DFSOutputStream&#36;ResponseProcessor.run(DFSClient.java:2638)<br /><br />11/08/10 06:33:07 WARN hdfs.DFSClient: DataStreamer Exception:<br />java.io.InterruptedIOException: Interruped while waiting for IO on channel<br />java.nio.channels.SocketChannel[connected local=/10.0.3.85:57316 remote=/<br />10.0.3.85:50010]. 484999 millis timeout left.<br />at<br />org.apache.hadoop.net.SocketIOWithTimeout&#36;SelectorPool.select(SocketIOWithTimeout.java:349)<br />at<br />org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:157)<br />at<br />org.apache.hadoop.net.SocketOutputStream.write(SocketOutputStream.java:146)<br />at<br />org.apache.hadoop.net.SocketOutputStream.write(SocketOutputStream.java:107)<br />at java.io.BufferedOutputStream.write(BufferedOutputStream.java:105)<br />at java.io.DataOutputStream.write(DataOutputStream.java:90)<br />at<br />org.apache.hadoop.hdfs.DFSClient&#36;DFSOutputStream&#36;DataStreamer.run(DFSClient.java:2526)<br /><br />11/08/10 06:33:07 INFO hdfs.DFSClient: Error Recovery for block<br />blk_156159297298887768_25457 waiting for responder to exit.<br />11/08/10 06:33:07 WARN hdfs.DFSClient: Error Recovery for block<br />blk_156159297298887768_25457 bad datanode[0] 10.0.3.85:50010<br />11/08/10 06:33:07 INFO mapred.JobClient: Cleaning up the staging area<br />hdfs://<br />hadoop.zoniversal.com/var/lib/hadoop-0.20/cache/mapred/mapred/staging/root/.staging/job_201107251124_0619<br />Aug 10, 2011 6:33:08 AM hbaseaggregator.AggregatorDriver main<br />SEVERE: null<br />java.io.IOException: All datanodes 10.0.3.85:50010 are bad. Aborting...<br />at<br />org.apache.hadoop.hdfs.DFSClient&#36;DFSOutputStream.processDatanodeError(DFSClient.java:2774)<br />at<br />org.apache.hadoop.hdfs.DFSClient&#36;DFSOutputStream.access&#36;1600(DFSClient.java:2305)<br />at<br />org.apache.hadoop.hdfs.DFSClient&#36;DFSOutputStream&#36;DataStreamer.run(DFSClient.java:2477)<br />11/08/10 06:33:08 ERROR hdfs.DFSClient: Exception closing file<br />/var/lib/hadoop-0.20/cache/mapred/mapred/staging/root/.staging/job_201107251124_0619/libjars/zookeeper-3.3.3-cdh3u0.jar<br />: java.io.IOException: All datanodes 10.0.3.85:50010 are bad. Aborting...<br />java.io.IOException: All datanodes 10.0.3.85:50010 are bad. Aborting...<br />at<br />org.apache.hadoop.hdfs.DFSClient&#36;DFSOutputStream.processDatanodeError(DFSClient.java:2774)<br />at<br />org.apache.hadoop.hdfs.DFSClient&#36;DFSOutputStream.access&#36;1600(DFSClient.java:2305)<br />at<br />org.apache.hadoop.hdfs.DFSClient&#36;DFSOutputStream&#36;DataStreamer.run(DFSClient.java:2477)<br /><br /><br />kindly let me know the solution.<br />thnx<br /><div class="ql ql1"><span class="qc_wrote">On Tue, Aug 9, 2011 at 8:38 PM, Ian Varley wrote:</span><br /><br />Hi Shuja,<br /><br />This question is mentioned in the HBase FAQ, here:<br /><br /><a href="http://wiki.apache.org/hadoop/Hbase/FAQ_Operations#A3" rel="nofollow">http://wiki.apache.org/hadoop/Hbase/FAQ_Operations#A3</a><br /><br />which points to the HBase book:<br /><br /><a href="http://hbase.apache.org/book.html#ulimit" rel="nofollow">http://hbase.apache.org/book.html#ulimit</a><br /><br />&quot;HBase is a database. It uses a lot of files all at the same time. The<br />default ulimit -n -- i.e. user file limit -- of 1024 on most *nix systems is<br />insufficient (On mac os x its 256). Any significant amount of loading will<br />lead you to &quot;FAQ: Why do I see &quot;java.io.IOException...(Too many open files)&quot;<br />in my logs?&quot; ... Do yourself a favor and change the upper bound on the<br />number of file descriptors. Set it to north of 10k. See the above referenced<br />FAQ for how. You should also up the hbase users&#39; nproc setting; under load,<br />a low-nproc setting could manifest as OutOfMemoryError. To be clear, upping<br />the file descriptors and nproc for the user who is running the HBase process<br />is an operating system configuration, not an HBase configuration. Also, a<br />common mistake is that administrators will up the file descriptors for a<br />particular user but for whatever reason, HBase will be running as some one<br />else. HBase prints in its logs as the first line the ulimit its seeing.<br />Ensure its correct.&quot;<br /><br />- Ian<br /><br />On Aug 9, 2011, at 10:32 AM, Shuja Rehman wrote:<br /><br />Hi All,<br /><br />I am running map reduce jobs in queue and after some jobs, the following<br />exception starts to come. Let me know if I need to change any settings or<br />anything else<br /><br />Thanks in advance.<br /><br /><br />11/08/09 06:19:56 INFO mapreduce.TableOutputFormat: Created table instance<br />for mytable<br />11/08/09 06:19:56 INFO zookeeper.ZooKeeper: Initiating client connection,<br />connectString=10.0.3.85:2181 sessionTimeout=180000 watcher=hconnection<br />11/08/09 06:19:56 ERROR mapreduce.TableInputFormat:<br />org.apache.hadoop.hbase.ZooKeeperConnectionException: java.io.IOException:<br />Too many open files<br />at<br /><br />org.apache.hadoop.hbase.client.HConnectionManager&#36;HConnectionImplementation.getZooKeeperWatcher(HConnectionManager.java:991)<br />at<br /><br />org.apache.hadoop.hbase.client.HConnectionManager&#36;HConnectionImplementation.setupZookeeperTrackers(HConnectionManager.java:302)<br />at<br /><br />org.apache.hadoop.hbase.client.HConnectionManager&#36;HConnectionImplementation.&lt;init&gt;(HConnectionManager.java:293)<br />at<br /><br />org.apache.hadoop.hbase.client.HConnectionManager.getConnection(HConnectionManager.java:156)<br />at org.apache.hadoop.hbase.client.HTable.&lt;init&gt;(HTable.java:167)<br />at org.apache.hadoop.hbase.client.HTable.&lt;init&gt;(HTable.java:145)<br />at<br /><br />org.apache.hadoop.hbase.mapreduce.TableInputFormat.setConf(TableInputFormat.java:91)<br />at<br />org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:62)<br />at<br /><br />org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:117)<br />at<br />org.apache.hadoop.mapred.JobClient.writeNewSplits(JobClient.java:941)<br />at<br />org.apache.hadoop.mapred.JobClient.writeSplits(JobClient.java:961)<br />at org.apache.hadoop.mapred.JobClient.access&#36;500(JobClient.java:170)<br />at org.apache.hadoop.mapred.JobClient&#36;2.run(JobClient.java:880)<br />at org.apache.hadoop.mapred.JobClient&#36;2.run(JobClient.java:833)<br />at java.security.AccessController.doPrivileged(Native Method)<br />at javax.security.auth.Subject.doAs(Subject.java:396)<br />at<br /><br />org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1115)<br />at<br />org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:833)<br />at org.apache.hadoop.mapreduce.Job.submit(Job.java:476)<br />at org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:506)<br />at<br />hbaseaggregator.AggregatorRunner.runJob(AggregatorRunner.java:242)<br />at<br />hbaseaggregator.AggregatorRunner.Aggregator(AggregatorRunner.java:180)<br />at hbaseaggregator.AggregatorDriver.main(AggregatorDriver.java:60)<br />at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)<br />at<br /><br />sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)<br />at<br /><br />sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)<br />at java.lang.reflect.Method.invoke(Method.java:597)<br />at org.apache.hadoop.util.RunJar.main(RunJar.java:186)<br />Caused by: java.io.IOException: Too many open files<br />at sun.nio.ch.IOUtil.initPipe(Native Method)<br />at sun.nio.ch.EPollSelectorImpl.&lt;init&gt;(EPollSelectorImpl.java:49)<br />at<br /><br />sun.nio.ch.EPollSelectorProvider.openSelector(EPollSelectorProvider.java:18)<br />at java.nio.channels.Selector.open(Selector.java:209)<br />at org.apache.zookeeper.ClientCnxn.&lt;init&gt;(ClientCnxn.java:160)<br />at org.apache.zookeeper.ClientCnxn.&lt;init&gt;(ClientCnxn.java:331)<br />at org.apache.zookeeper.ZooKeeper.&lt;init&gt;(ZooKeeper.java:377)<br />at org.apache.hadoop.hbase.zookeeper.ZKUtil.connect(ZKUtil.java:97)<br />at<br /><br />org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.&lt;init&gt;(ZooKeeperWatcher.java:119)<br />at<br /><br />org.apache.hadoop.hbase.client.HConnectionManager&#36;HConnectionImplementation.getZooKeeperWatcher(HConnectionManager.java:989)<br />... 27 more<br /><br />11/08/09 06:19:56 INFO mapred.JobClient: Cleaning up the staging area<br />hdfs://<br /><br />hadoop.zoniversal.com/var/lib/hadoop-0.20/cache/mapred/mapred/staging/root/.staging/job_201107251124_0383<br />&lt;<br /><a href="http://hadoop.zoniversal.com/var/lib/hadoop-0.20/cache/mapred/mapred/staging/root/.staging/job_201107251124_0383" rel="nofollow">http://hadoop.zoniversal.com/var/lib/hadoop-0.20/cache/mapred/mapred/staging/root/.staging/job_201107251124_0383</a><br /><div class="ql ql2"></div>Exception in thread &quot;main&quot; java.lang.InternalError<br />at<br />sun.misc.URLClassPath&#36;JarLoader.getResource(URLClassPath.java:755)<br />at sun.misc.URLClassPath.getResource(URLClassPath.java:169)<br />at java.net.URLClassLoader&#36;1.run(URLClassLoader.java:194)<br />at java.security.AccessController.doPrivileged(Native Method)<br />at java.net.URLClassLoader.findClass(URLClassLoader.java:190)<br />at sun.misc.Launcher&#36;ExtClassLoader.findClass(Launcher.java:229)<br />at java.lang.ClassLoader.loadClass(ClassLoader.java:307)<br />at java.lang.ClassLoader.loadClass(ClassLoader.java:296)<br />at sun.misc.Launcher&#36;AppClassLoader.loadClass(Launcher.java:301)<br />at java.lang.ClassLoader.loadClass(ClassLoader.java:248)<br />at<br />java.util.ResourceBundle&#36;RBClassLoader.loadClass(ResourceBundle.java:435)<br />at<br />java.util.ResourceBundle&#36;Control.newBundle(ResourceBundle.java:2289)<br />at java.util.ResourceBundle.loadBundle(ResourceBundle.java:1364)<br />at java.util.ResourceBundle.findBundle(ResourceBundle.java:1328)<br />at java.util.ResourceBundle.findBundle(ResourceBundle.java:1282)<br />at java.util.ResourceBundle.getBundleImpl(ResourceBundle.java:1224)<br />at java.util.ResourceBundle.getBundle(ResourceBundle.java:705)<br />at java.util.logging.Level.getLocalizedName(Level.java:223)<br />at java.util.logging.SimpleFormatter.format(SimpleFormatter.java:64)<br />at java.util.logging.StreamHandler.publish(StreamHandler.java:179)<br />at java.util.logging.ConsoleHandler.publish(ConsoleHandler.java:88)<br />at java.util.logging.Logger.log(Logger.java:458)<br />at java.util.logging.Logger.doLog(Logger.java:480)<br />at java.util.logging.Logger.log(Logger.java:569)<br />at hbaseaggregator.AggregatorDriver.main(AggregatorDriver.java:72)<br />at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)<br />at<br /><br />sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)<br />at<br /><br />sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)<br />at java.lang.reflect.Method.invoke(Method.java:597)<br />at org.apache.hadoop.util.RunJar.main(RunJar.java:186)<br />Caused by: java.util.zip.ZipException: error in opening zip file<br />at java.util.zip.ZipFile.open(Native Method)<br />at java.util.zip.ZipFile.&lt;init&gt;(ZipFile.java:114)<br />at java.util.jar.JarFile.&lt;init&gt;(JarFile.java:135)<br />at java.util.jar.JarFile.&lt;init&gt;(JarFile.java:72)<br />at sun.misc.URLClassPath&#36;JarLoader.getJarFile(URLClassPath.java:646)<br />at sun.misc.URLClassPath&#36;JarLoader.access&#36;600(URLClassPath.java:540)<br />at sun.misc.URLClassPath&#36;JarLoader&#36;1.run(URLClassPath.java:607)<br />at java.security.AccessController.doPrivileged(Native Method)<br />at sun.misc.URLClassPath&#36;JarLoader.ensureOpen(URLClassPath.java:599)<br />at<br />sun.misc.URLClassPath&#36;JarLoader.getResource(URLClassPath.java:753)<br />... 29 more<br /><br />--<br />Regards<br />Shuja-ur-Rehman Baig<br /><<a href="http://pk.linkedin.com/in/shujamughal" rel="nofollow">http://pk.linkedin.com/in/shujamughal</a>><br /></div><br />--<br />Regards<br />Shuja-ur-Rehman Baig<br /><<a href="http://pk.linkedin.com/in/shujamughal" rel="nofollow">http://pk.linkedin.com/in/shujamughal</a>></div></div>				<div style="margin-top:1em">
					<a href="mailto:user%40hbase.apache.org?In-Reply-To=%3CCADcMMgFAgYDvdvhVVcA65HRGPxr0KAPaw%3DN7NL0gJtsvbc71pQ%40mail.gmail.com%3E&Subject=Re%3A%20org.apache.hadoop.hbase.ZooKeeperConnectionException%3A%20java.io.IOException%3A%20Too%20many%20open%20files">reply</a> <span class="inactive">|</span> <a href="/p/hbase/user/118ajbp9vs/org-apache-hadoop-hbase-zookeeperconnectionexception-java-io-ioexception-too-many-open-files">permalink</a>
				</div>
			</div>

		</div><!-- so.article -->
		<div style="clear:both"></div>
		</li>

		</ul><div style="clear:both"></div><div style="clear:both;height:1em"></div><div><h2>Related Discussions</h2></div><div><div id="related_discussions" style="padding-bottom:1em"><ul class="bullet">				<li style="margin-bottom:0.3em"><span><a class="hl_y" style="font-weight:bold" href="/t/hbase/user/11569trjk1/mapreduce-job-reading-directly-from-the-hbase-files-in-hdfs" title="Is there an issue open or any particular reason that an MR job needs to access the HBase data directly from the region server? It seems possible to also provide functionality such that MR can execute ...">MapReduce job reading directly from the HBase files in HDFS</a></span></li>
							<li style="margin-bottom:0.3em"><span><a class="hl_y" style="font-weight:bold" href="/t/hbase/user/111bqp9neg/java-net-socketexception-too-many-open-files" title="I set the env as fallows: $ ulimit -n 65535 $ ulimit -a core file size (blocks, -c) 0 data seg size (kbytes, -d) unlimited scheduling priority (-e) 0 file size (blocks, -f) unlimited pending signals ...">java.net.SocketException: Too many open files</a></span></li>
							<li style="margin-bottom:0.3em"><span><a class="hl_y" style="font-weight:bold" href="/t/hbase/user/10cdst4yae/hbase-stability" title="Hi all! We have been using HBase 0.20.4 (cdh3b1) in production on 2 nodes for a few months now and we are having constant issues with it. We fell over all standard traps (like &quot;Too many open files&quot;, ...">HBase stability</a></span></li>
							<li style="margin-bottom:0.3em"><span><a class="hl_y" style="font-weight:bold" href="/t/hbase/user/10bcb1tecc/worried-about-noserverforregionexception-no-server-address-listed-in-meta-for-region" title="Hi again, we have a 1 master, 3 data nodes Hadoop+HBase cluster for a PoC. I ran into &quot;Too many open files&quot; errors on the region server during load testing. No problem as such. But now, after ...">Worried about NoServerForRegionException: No server address listed in .META. for region</a></span></li>
							<li style="margin-bottom:0.3em"><span><a class="hl_y" style="font-weight:bold" href="/t/hbase/user/106e7epzf3/too-many-open-files" title="Please don&#39;t email the &#39;issues&#39; list. http://wiki.apache.org/hadoop/Hbase/Troubleshooting#A6 2010/6/14 chen peng &lt;chenpeng0122@hotmail.com :">Re: Too many open files</a></span></li>
							<li style="margin-bottom:0.3em"><span><a class="hl_y" style="font-weight:bold" href="/t/hbase/user/092qbhe02a/embarrassing-question-on-open-files-limit" title="Hi, all As I understand, if I change the number of open files in /etc/security/limits.conf I have to _reboot_ the computer. Is it correct? Thank you for your cooperation, M.">Embarrassing question on open files limit</a></span></li>
							<li style="margin-bottom:0.3em"><span><a class="hl_y" style="font-weight:bold" href="/t/hbase/user/0923rz919e/hbase-exceptions" title="Hi, all We ran an HBase cluster of (1 master/name node + 3 region server/data nodes). We upped the number of open files per process, increased the heap size of the region servers and data nodes to ...">Hbase Exceptions</a></span></li>
							<li style="margin-bottom:0.3em"><span><a class="hl_y" style="font-weight:bold" href="/t/hbase/user/091vqxq31e/question-on-hdfs-corruption" title="Hi, all When we ran an Hbase cluster w/o upping open files limit, the data nodes failed with &quot;too many open files&quot; exception. As a result, the entire HDFS file system is corrupted now. Does anybody ...">Question on HDFS corruption</a></span></li>
							<li style="margin-bottom:0.3em"><span><a class="hl_y" style="font-weight:bold" href="/t/hbase/user/091m18h8cb/too-many-open-files" title="Hi, all It looks like that one of our region servers run out of file descriptors, cannot open an epoll and shut down (see exception below). I read the FAQ and will increase the number of file ...">Too many open files</a></span></li>
							<li style="margin-bottom:0.3em"><span><a class="hl_y" style="font-weight:bold" href="/t/hbase/user/1191174e84/distributed-cache" title="Using distributed cache i put a common file in the hdfs.It contains of frequent files to remove.In the code i converted words in the table into a hashtable and removed words from other documents if ...">Distributed Cache</a></span></li>
			</ul></div></div>
			</div><!-- main_1 -->
	<div class="main_2"><!-- class="main_2"-->


		<div class="sb_box">
			<div class="sb_box_header">Discussion Navigation</div>
			<table>
				<tr><td class="sb_tbl_key">view</td><td class="sb_tbl_val"><span class="current">thread</a> <span class="inactive">|</span> <a href="/p/hbase/user/118902mpra/org-apache-hadoop-hbase-zookeeperconnectionexception-java-io-ioexception-too-many-open-files">post</a></td></tr>
		</table></div>
		<div class="sb_box">
			<div class="sb_box_header">Discussion Overview</div>
			<table>
				<tr><td class="sb_tbl_key">group</td><td class="sb_tbl_val"><a href="/g/hbase/user">user</a> <a style="display:none" href="/g/hbase">@<br />
<b>Notice</b>:  Undefined variable: pl_domain_short in <b>/home/whirl/sites/grokbase/root/www/public_html__www/cc/flow/tpc.php</b> on line <b>1543</b><br />
</a>
					<a href="/g/hbase/user/info"><img src="/cc/img/set/icon_question-mark_11x11.png" /></a>					</td></tr>
				<tr><td class="sb_tbl_key">categories</td><td class="sb_tbl_val"><a href="/s/hbase">hbase</a>, <a href="/s/hadoop">hadoop</a></td></tr>				<tr><td class="sb_tbl_key">posted</td><td class="sb_tbl_val"><a class="dt_y2" href="/g/hbase/user/2011/08">Aug 9, '11 at 3:32p</a></td></tr>
				<tr><td class="sb_tbl_key">active</td><td class="sb_tbl_val"><a class="dt_y2" href="/g/hbase/user/2011/08">Aug 10, '11 at 3:56p</a></td></tr>
				<tr><td class="sb_tbl_key">posts</td><td class="sb_tbl_val">4</td></tr>
				<tr><td class="sb_tbl_key">users</td><td class="sb_tbl_val">3</td></tr>
				<tr><td class="sb_tbl_key">website</td><td class="sb_tbl_val"><a class="external" href="http://hbase.apache.org">hbase.apache.org</a></td></tr>
						</table>
		</div>
		<div class="sb_box" id="sb_preferences" style="display:none" style="display:none" onmouseover="$('.sb_pref_hints').css('display','');" onmouseout="$('.sb_pref_hints').css('display','none');">
			<div class="sb_box_header">Preferences</div>
		
			<table style="width:100%">
				<tr><td class="sb_tbl_key">responses</td>
					<td class="sb_tbl_val"><a id="ctl_rbody_showhide" href="#" onclick="if ($('div.rbody').css('display') == 'none') {$('.tr_head_thin').css('display','none');$('.rbody').css('display','');$('#ctl_rbody_showhide').html('expanded');setCookie('rbody','s')} else {$('.rbody').css('display','none');$('.tr_head_thin').css('display','');$('#ctl_rbody_showhide').html('collapsed');setCookie('rbody','h')}; return false;">expanded</a>
				</td>
				<td class="sb_tbl_val sb_tbl_rt inactive sb_pref_hints" style="display:none">Hotkey:&nbsp;s</td></tr>
				<tr><td class="sb_tbl_key">font</td>
					<td class="sb_tbl_val"><a id="ctl_font" href="#" onclick="if ( $('div.m_body').css('font-family').match(/monospace/) ) { $('div.m_body').css('font-family','');$('div.m_body').css('font-size','90%');$('#ctl_font').html('variable');setCookie('font','v') } else { $('div.m_body').css('font-family','DejaVu Sans Mono,Lucida Console,Monaco,courier new,courier,monospace');$('div.m_body').css('font-size','80%');$('#ctl_font').html('fixed');setCookie('font','f') } ;return false;">variable</a>
				</td>
				<td class="sb_tbl_val sb_tbl_rt inactive sb_pref_hints" style="display:none">Hotkey:&nbsp;f</td></tr>


				<tr><td class="sb_tbl_key">user style</td>
				<td class="sb_tbl_val"><a id="ctl_ulist_style" href="#" onclick="uPrefs.tog_sb_u_style();return false">avatars</a></td>
				<td class="sb_tbl_val sb_tbl_rt inactive sb_pref_hints" style="display:none">Hotkey:&nbsp;a</td></tr>
			</table>
		</div>


		<div id="user_list_content">
			<h3>3 users in discussion</h3>
			<div id="user_list_avatars" style="width:100%;padding-left:10px">
		
			<a href="/user/Shuja-Rehman/Yf1fAmN5vPScoUqpokfi3Y" title="Shuja Rehman: 2 posts"><img class="av_40" src="/avatar/Yf1fAmN5vPScoUqpokfi3Y/40" alt="Shuja Rehman: 2 posts" width="40" height="40" /></a>

			
			<a href="/user/Ian-Varley/3BtKyz56CyauKGRrdHYYm3" title="Ian Varley: 1 post"><img class="av_40" src="/avatar/3BtKyz56CyauKGRrdHYYm3/40" alt="Ian Varley: 1 post" width="40" height="40" /></a>

			
			<a href="/user/Stack/jzPsrKPBmnffhPqbsNH3ST" title="Stack: 1 post"><img class="av_40" src="/avatar/jzPsrKPBmnffhPqbsNH3ST/40" alt="Stack: 1 post" width="40" height="40" /></a>

						</div><div id="user_list_names" class="hl_y" style="display:none">
					<a style="margin-top:-3px" href="/user/Shuja-Rehman/Yf1fAmN5vPScoUqpokfi3Y" title="Shuja Rehman: 2 posts">Shuja Rehman (2)</a><br />
						<a style="margin-top:-3px" href="/user/Ian-Varley/3BtKyz56CyauKGRrdHYYm3" title="Ian Varley: 1 post">Ian Varley (1)</a><br />
						<a style="margin-top:-3px" href="/user/Stack/jzPsrKPBmnffhPqbsNH3ST" title="Stack: 1 post">Stack (1)</a><br />
						</div><div style="clear:both;height:1em"></div>
		</div>
		
					</div><!-- sb/main_2 -->
<script type="text/javascript">
//<![CDATA[
var dt=new dateTime;dt.init();$('a.dt_y2').each(function(index){var dtl=dt.local_text_for_any($(this).html(),1,1);if(dtl.length)$(this).html(dtl);});
$('.dt_y4').each(function(index){var dtl=dt.local_text_for_any($(this).html());if(dtl.length)$(this).html(dtl);});

$(document).bind('keydown','s',function(){if ($('div.rbody').css('display') == 'none') {$('.tr_head_thin').css('display','none');$('.rbody').css('display','');$('#ctl_rbody_showhide').html('expanded');setCookie('rbody','s')} else {$('.rbody').css('display','none');$('.tr_head_thin').css('display','');$('#ctl_rbody_showhide').html('collapsed');setCookie('rbody','h')}});
$(document).bind('keydown','d',function(){uPrefs.tog_sb_u_list()});
$(document).bind('keydown','a',function(){uPrefs.tog_sb_u_style()});
$(document).bind('keydown','f',function(){if($('div.m_body').css('font-family').match(/monospace/) ) { $('div.m_body').css('font-family','');$('div.m_body_one').css('font-size','100%');$('div.m_body_resp').css('font-size','90%');$('#ctl_font').html('variable');setCookie('font','v') } else { $('div.m_body').css('font-family','DejaVu Sans Mono,Lucida Console,Monaco,courier new,courier,monospace');$('div.m_body').css('font-size','90%');$('#ctl_font').html('fixed');setCookie('font','f')};});

$('#sb_preferences').css('display','');
//var tPgFwd    = new pgFwd(,); tPgFwd.init();
var iRToggler = new rToggler;iRToggler.init();
var iQToggler = new qToggler;iQToggler.init("t");
if (getCookie('font')=='f') {$('div.m_body').css('font-family','DejaVu Sans Mono,Lucida Console,Monaco,courier new,courier,monospace');$('div.m_body').css('font-size','80%');$('#ctl_font').html('fixed');};
var uPrefs = new user_prefs('t');uPrefs.init();
//if (getCookie('ustyle')=='n')   {$('#user_list_avatars').css('display','none');$('#user_list_names').css('display','');$('#ctl_ulist_style').html('names');};
//]]>
</script>

</div><!-- main -->
<div style="clear:both"></div>

	<div style="clear:both;height:1em"></div>

</div><!-- main -->
<div id="footer_sep"></div>
<div id="footer">
	<div id="footer_content">
	<div id="footer_col1">
		<h3>Content</h3>
		<ul class="bullet">
			<li><a href="/">Home</a></li>
			<li><a href="/groups">Groups &amp; Organizations</a></li>
			<li><a href="/events">Events</a></li>
		</ul>
	</div>
	<div id="footer_col2">
		<h3>People</h3>
		<ul class="bullet">
			<li><a href="/users">Users</a></li>
			<li><a href="/badges">Badges</a></li>
		</ul>
	</div>
	<div id="footer_col3">
		<h3>Support</h3>
		<ul class="bullet">
			<li><a href="/welcome">Welcome</a></li>
			<li><a href="/faq">F.A.Q.</a></li>
			<li><a href="/faq#miscellaneous__contact">Contact Us</a></li>
		</ul>
	</div>
	<div id="footer_coln">

		<h3>Translate</h3>

<div id="google_translate_element"></div><script>
function googleTranslateElementInit() {
  new google.translate.TranslateElement({
    pageLanguage: 'auto',
    autoDisplay: false,
    layout: google.translate.TranslateElement.InlineLayout.SIMPLE
  }, 'google_translate_element');
}
</script><script src="//translate.google.com/translate_a/element.js?cb=googleTranslateElementInit"></script>

	</div>

	<div style="clear:both"></div>

	<p id="copy">site design / logo &copy; 2013 Grokbase</p>
	</div>
</div>
</div>
<script type="text/javascript">
$('.dh').show();
</script>
</body></html>
