<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <title>Re: DirectoryScanner's OutOfMemoryError</title>
  <link rel="stylesheet" type="text/css" href="/archives/style.css" />
 </head>

 <body id="archives">
  <h1>hadoop-hdfs-user mailing list archives</h1>

  <h5>
<a href="http://mail-archives.apache.org/mod_mbox/" title="Back to the archives depot">Site index</a> &middot; <a href="/mod_mbox/hadoop-hdfs-user" title="Back to the list index">List index</a></h5>  <table class="static" id="msgview">
   <thead>
    <tr>
    <th class="title">Message view</th>
    <th class="nav"><a href="/mod_mbox/hadoop-hdfs-user/201306.mbox/%3cCDD76FB5.E664%25nniemeyer@riotgames.com%3e" title="Previous by date">&laquo;</a> <a href="/mod_mbox/hadoop-hdfs-user/201306.mbox/date" title="View messages sorted by date">Date</a> <a href="/mod_mbox/hadoop-hdfs-user/201306.mbox/%3cCAOcnVr3VsEem0g997Q7p18vjavptErp1oJR7OvvCdL4TicO63Q@mail.gmail.com%3e" title="Next by date">&raquo;</a> &middot; <a href="/mod_mbox/hadoop-hdfs-user/201306.mbox/%3cCAG1CohSchsAOXQWa9_1r=rPo_CSTMMBC1ZXug2fnoYhJvJZYew@mail.gmail.com%3e" title="Previous by thread">&laquo;</a> <a href="/mod_mbox/hadoop-hdfs-user/201306.mbox/thread" title="View messages sorted by thread">Thread</a> <a href="/mod_mbox/hadoop-hdfs-user/201306.mbox/%3cCAO7hTbO64D96DAUqPhasT-=K=4NSERjyHAnCYnfi7_oshHvqsA@mail.gmail.com%3e" title="Next by thread">&raquo;</a></th>
   </tr>
   </thead>

   <tfoot>
    <tr>
    <th class="title"><a href="#archives">Top</a></th>
    <th class="nav"><a href="/mod_mbox/hadoop-hdfs-user/201306.mbox/%3cCDD76FB5.E664%25nniemeyer@riotgames.com%3e" title="Previous by date">&laquo;</a> <a href="/mod_mbox/hadoop-hdfs-user/201306.mbox/date" title="View messages sorted by date">Date</a> <a href="/mod_mbox/hadoop-hdfs-user/201306.mbox/%3cCAOcnVr3VsEem0g997Q7p18vjavptErp1oJR7OvvCdL4TicO63Q@mail.gmail.com%3e" title="Next by date">&raquo;</a> &middot; <a href="/mod_mbox/hadoop-hdfs-user/201306.mbox/%3cCAG1CohSchsAOXQWa9_1r=rPo_CSTMMBC1ZXug2fnoYhJvJZYew@mail.gmail.com%3e" title="Previous by thread">&laquo;</a> <a href="/mod_mbox/hadoop-hdfs-user/201306.mbox/thread" title="View messages sorted by thread">Thread</a> <a href="/mod_mbox/hadoop-hdfs-user/201306.mbox/%3cCAO7hTbO64D96DAUqPhasT-=K=4NSERjyHAnCYnfi7_oshHvqsA@mail.gmail.com%3e" title="Next by thread">&raquo;</a></th>
   </tr>
   </tfoot>

   <tbody>
   <tr class="from">
    <td class="left">From</td>
    <td class="right">Harsh J &lt;ha...@cloudera.com&gt;</td>
   </tr>
   <tr class="subject">
    <td class="left">Subject</td>
    <td class="right">Re: DirectoryScanner's OutOfMemoryError</td>
   </tr>
   <tr class="date">
    <td class="left">Date</td>
    <td class="right">Fri, 07 Jun 2013 16:24:44 GMT</td>
   </tr>
   <tr class="contents"><td colspan="2"><pre>
Please see https://issues.apache.org/jira/browse/HDFS-4461. You may
have to raise your heap for DN if you've accumulated a lot of blocks
per DN.

On Fri, Jun 7, 2013 at 8:33 PM, YouPeng Yang &lt;yypvsxf19870706@gmail.com&gt; wrote:
&gt; Hi All
&gt;
&gt;    I have found that the DirectoryScanner gets error:  Error compiling
&gt; report because of java.lang.OutOfMemoryError: Java heap space.
&gt;   The log details are as [1]:
&gt;
&gt;   How does the error come out ,and how to solve this exception?
&gt;
&gt;
&gt; [1]
&gt; 2013-06-07 22:20:28,199 INFO
&gt; org.apache.hadoop.hdfs.server.datanode.DataNode: Took 2737ms to process 1
&gt; commands from NN
&gt; 2013-06-07 22:20:28,199 INFO
&gt; org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService:
&gt; Deleted block BP-471453121-172.16.250.16-1369298226760
&gt; blk_1870928037426403148_709040 at file
&gt; /home/hadoop/datadir/current/BP-471453121-172.16.250.16-1369298226760/current/finalized/subdir24/subdir25/blk_1870928037426403148
&gt; 2013-06-07 22:20:28,199 INFO
&gt; org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService:
&gt; Deleted block BP-471453121-172.16.250.16-1369298226760
&gt; blk_3693882010743127822_709044 at file
&gt; /home/hadoop/datadir/current/BP-471453121-172.16.250.16-1369298226760/current/finalized/subdir24/subdir25/blk_3693882010743127822
&gt; 2013-06-07 22:20:28,743 INFO
&gt; org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService:
&gt; Deleted block BP-471453121-172.16.250.16-1369298226760
&gt; blk_-5452984265504491579_709036 at file
&gt; /home/hadoop/datadir/current/BP-471453121-172.16.250.16-1369298226760/current/finalized/subdir24/subdir25/blk_-5452984265504491579
&gt; 2013-06-07 22:20:28,743 INFO
&gt; org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService:
&gt; Deleted block BP-471453121-172.16.250.16-1369298226760
&gt; blk_-1078215880381545528_709050 at file
&gt; /home/hadoop/datadir/current/BP-471453121-172.16.250.16-1369298226760/current/finalized/subdir24/subdir25/blk_-1078215880381545528
&gt; 2013-06-07 22:20:28,744 INFO
&gt; org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService:
&gt; Deleted block BP-471453121-172.16.250.16-1369298226760
&gt; blk_8107220088215975918_709064 at file
&gt; /home/hadoop/datadir/current/BP-471453121-172.16.250.16-1369298226760/current/finalized/subdir24/subdir25/blk_8107220088215975918
&gt; 2013-06-07 22:20:29,278 INFO
&gt; org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService:
&gt; Deleted block BP-471453121-172.16.250.16-1369298226760
&gt; blk_-3527717187851336238_709052 at file
&gt; /home/hadoop/datadir/current/BP-471453121-172.16.250.16-1369298226760/current/finalized/subdir24/subdir25/blk_-3527717187851336238
&gt; 2013-06-07 22:20:29,812 INFO
&gt; org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService:
&gt; Deleted block BP-471453121-172.16.250.16-1369298226760
&gt; blk_1766998327682981895_709042 at file
&gt; /home/hadoop/datadir/current/BP-471453121-172.16.250.16-1369298226760/current/finalized/subdir24/subdir25/blk_1766998327682981895
&gt; 2013-06-07 22:20:29,812 INFO
&gt; org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService:
&gt; Deleted block BP-471453121-172.16.250.16-1369298226760
&gt; blk_1650592414141359061_709028 at file
&gt; /home/hadoop/datadir/current/BP-471453121-172.16.250.16-1369298226760/current/finalized/subdir24/subdir25/blk_1650592414141359061
&gt; 2013-06-07 22:20:29,812 INFO
&gt; org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService:
&gt; Deleted block BP-471453121-172.16.250.16-1369298226760
&gt; blk_-6527697040536951940_709038 at file
&gt; /home/hadoop/datadir/current/BP-471453121-172.16.250.16-1369298226760/current/finalized/subdir24/subdir25/blk_-6527697040536951940
&gt; 2013-06-07 22:20:43,766 ERROR
&gt; org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Error compiling
&gt; report
&gt; java.util.concurrent.ExecutionException: java.lang.OutOfMemoryError: Java
&gt; heap space
&gt; at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
&gt; at java.util.concurrent.FutureTask.get(FutureTask.java:83)
&gt; at
&gt; org.apache.hadoop.hdfs.server.datanode.DirectoryScanner.getDiskReport(DirectoryScanner.java:468)
&gt; at
&gt; org.apache.hadoop.hdfs.server.datanode.DirectoryScanner.scan(DirectoryScanner.java:349)
&gt; at
&gt; org.apache.hadoop.hdfs.server.datanode.DirectoryScanner.reconcile(DirectoryScanner.java:330)
&gt; at
&gt; org.apache.hadoop.hdfs.server.datanode.DirectoryScanner.run(DirectoryScanner.java:286)
&gt; at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
&gt; at
&gt; java.util.concurrent.FutureTask$Sync.innerRunAndReset(FutureTask.java:317)
&gt; at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:150)
&gt; at
&gt; java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$101(ScheduledThreadPoolExecutor.java:98)
&gt; at
&gt; java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.runPeriodic(ScheduledThreadPoolExecutor.java:180)
&gt; at
&gt; java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:204)
&gt; at
&gt; java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
&gt; at
&gt; java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
&gt; at java.lang.Thread.run(Thread.java:662)
&gt; Caused by: java.lang.OutOfMemoryError: Java heap space
&gt; at java.util.Arrays.copyOf(Arrays.java:2882)
&gt; at
&gt; java.lang.AbstractStringBuilder.expandCapacity(AbstractStringBuilder.java:100)
&gt; at java.lang.AbstractStringBuilder.append(AbstractStringBuilder.java:572)
&gt; at java.lang.StringBuilder.append(StringBuilder.java:203)
&gt; at java.io.UnixFileSystem.resolve(UnixFileSystem.java:93)
&gt; at java.io.File.&lt;init&gt;(File.java:207)
&gt; at java.io.File.listFiles(File.java:1056)
&gt; at org.apache.hadoop.fs.FileUtil.listFiles(FileUtil.java:730)
&gt; at
&gt; org.apache.hadoop.hdfs.server.datanode.DirectoryScanner$ReportCompiler.compileReport(DirectoryScanner.java:518)
&gt; at
&gt; org.apache.hadoop.hdfs.server.datanode.DirectoryScanner$ReportCompiler.compileReport(DirectoryScanner.java:533)
&gt; at
&gt; org.apache.hadoop.hdfs.server.datanode.DirectoryScanner$ReportCompiler.compileReport(DirectoryScanner.java:533)
&gt; at
&gt; org.apache.hadoop.hdfs.server.datanode.DirectoryScanner$ReportCompiler.call(DirectoryScanner.java:508)
&gt; at
&gt; org.apache.hadoop.hdfs.server.datanode.DirectoryScanner$ReportCompiler.call(DirectoryScanner.java:493)
&gt; at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
&gt; at java.util.concurrent.FutureTask.run(FutureTask.java:138)
&gt; ... 3 more
&gt; 2013-06-07 22:20:43,994 ERROR
&gt; org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Exception during
&gt; DirectoryScanner execution - will continue next cycle
&gt; java.lang.RuntimeException: java.util.concurrent.ExecutionException:
&gt; java.lang.OutOfMemoryError: Java heap space
&gt; at
&gt; org.apache.hadoop.hdfs.server.datanode.DirectoryScanner.getDiskReport(DirectoryScanner.java:472)
&gt; at
&gt; org.apache.hadoop.hdfs.server.datanode.DirectoryScanner.scan(DirectoryScanner.java:349)
&gt; at
&gt; org.apache.hadoop.hdfs.server.datanode.DirectoryScanner.reconcile(DirectoryScanner.java:330)
&gt; at
&gt; org.apache.hadoop.hdfs.server.datanode.DirectoryScanner.run(DirectoryScanner.java:286)
&gt; at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
&gt; at
&gt; java.util.concurrent.FutureTask$Sync.innerRunAndReset(FutureTask.java:317)
&gt; at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:150)
&gt; at
&gt; java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$101(ScheduledThreadPoolExecutor.java:98)
&gt; at
&gt; java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.runPeriodic(ScheduledThreadPoolExecutor.java:180)
&gt; at
&gt; java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:204)
&gt; at
&gt; java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
&gt; at
&gt; java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
&gt; at java.lang.Thread.run(Thread.java:662)
&gt; Caused by: java.util.concurrent.ExecutionException:
&gt; java.lang.OutOfMemoryError: Java heap space
&gt; at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
&gt; at java.util.concurrent.FutureTask.get(FutureTask.java:83)
&gt; at
&gt; org.apache.hadoop.hdfs.server.datanode.DirectoryScanner.getDiskReport(DirectoryScanner.java:468)
&gt; ... 12 more
&gt; Caused by: java.lang.OutOfMemoryError: Java heap space
&gt; at java.util.Arrays.copyOf(Arrays.java:2882)
&gt; at
&gt; java.lang.AbstractStringBuilder.expandCapacity(AbstractStringBuilder.java:100)
&gt; at java.lang.AbstractStringBuilder.append(AbstractStringBuilder.java:572)
&gt; at java.lang.StringBuilder.append(StringBuilder.java:203)
&gt; at java.io.UnixFileSystem.resolve(UnixFileSystem.java:93)
&gt; at java.io.File.&lt;init&gt;(File.java:207)
&gt; at java.io.File.listFiles(File.java:1056)
&gt; at org.apache.hadoop.fs.FileUtil.listFiles(FileUtil.java:730)
&gt; at
&gt; org.apache.hadoop.hdfs.server.datanode.DirectoryScanner$ReportCompiler.compileReport(DirectoryScanner.java:518)
&gt; at
&gt; org.apache.hadoop.hdfs.server.datanode.DirectoryScanner$ReportCompiler.compileReport(DirectoryScanner.java:533)
&gt; at
&gt; org.apache.hadoop.hdfs.server.datanode.DirectoryScanner$ReportCompiler.compileReport(DirectoryScanner.java:533)
&gt; at
&gt; org.apache.hadoop.hdfs.server.datanode.DirectoryScanner$ReportCompiler.call(DirectoryScanner.java:508)
&gt; at
&gt; org.apache.hadoop.hdfs.server.datanode.DirectoryScanner$ReportCompiler.call(DirectoryScanner.java:493)
&gt; at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
&gt; at java.util.concurrent.FutureTask.run(FutureTask.java:138)
&gt; ... 3 more
&gt;
&gt;
&gt;
&gt;
&gt;
&gt; 2013-06-07 22:30:05,859 INFO
&gt; org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving block
&gt; BP-471453121-172.16.250.16-1369298226760:blk_9041774691760522723_709080 src:
&gt; /172.16.250.15:54457 dest: /172.16.250.17:50010
&gt; 2013-06-07 22:30:05,885 INFO
&gt; org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src:
&gt; /172.16.250.15:54457, dest: /172.16.250.17:50010, bytes: 224277, op:
&gt; HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1691104063_1, offset: 0, srvID:
&gt; DS-869018356-172.16.250.17-50010-1369298284382, blockid:
&gt; BP-471453121-172.16.250.16-1369298226760:blk_9041774691760522723_709080,
&gt; duration: 24379944
&gt; 2013-06-07 22:30:05,886 INFO
&gt; org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder:
&gt; BP-471453121-172.16.250.16-1369298226760:blk_9041774691760522723_709080,
&gt; type=LAST_IN_PIPELINE, downstreams=0:[] terminating
&gt; 2013-06-07 22:30:05,983 INFO
&gt; org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving block
&gt; BP-471453121-172.16.250.16-1369298226760:blk_-7095462972895018344_709084
&gt; src: /172.16.250.18:49485 dest: /172.16.250.17:50010
&gt;
&gt;
&gt;
&gt;



--
Harsh J

</pre></td></tr>
   <tr class="mime">
    <td class="left">Mime</td>
    <td class="right">
<ul>
<li><a rel="nofollow" href="/mod_mbox/hadoop-hdfs-user/201306.mbox/raw/%3cCAOcnVr0g1-kL84xKOd6WBTV_JpEwtCrOJKJ1DcWCCcXBjSB5ew@mail.gmail.com%3e/">Unnamed text/plain</a> (inline, None, 10740 bytes)</li>
</ul>
</td>
</tr>
   <tr class="raw">
    <td class="left"></td>
    <td class="right"><a href="/mod_mbox/hadoop-hdfs-user/201306.mbox/raw/%3cCAOcnVr0g1-kL84xKOd6WBTV_JpEwtCrOJKJ1DcWCCcXBjSB5ew@mail.gmail.com%3e" rel="nofollow">View raw message</a></td>
   </tr>
   </tbody>
  </table>
 </body>
</html>
